{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-09T12:30:57.214424Z",
     "start_time": "2018-07-09T12:05:40.750808Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 461\n",
      "1 888\n",
      "2 1326\n",
      "3 1766\n",
      "4 2221\n",
      "5 2656\n",
      "6 3117\n",
      "7 3590\n",
      "8 3972\n",
      "9 4412\n",
      "10 4844\n",
      "11 5302\n",
      "12 5718\n",
      "13 6148\n",
      "14 6602\n",
      "15 6990\n",
      "16 7470\n",
      "17 7888\n",
      "18 8380\n",
      "19 8811\n",
      "20 9278\n",
      "21 9701\n",
      "22 10126\n",
      "23 10587\n",
      "24 11016\n",
      "25 11443\n",
      "26 11912\n",
      "27 12381\n",
      "28 12834\n",
      "29 13239\n",
      "30 13641\n",
      "31 14105\n",
      "32 14527\n",
      "33 14981\n",
      "34 15419\n",
      "35 15865\n",
      "36 16277\n",
      "37 16736\n",
      "38 17170\n",
      "39 17602\n",
      "40 18035\n",
      "41 18460\n",
      "42 18882\n",
      "43 19345\n",
      "44 19737\n",
      "45 20176\n",
      "46 20653\n",
      "47 21079\n",
      "48 21503\n",
      "49 21930\n",
      "50 22361\n",
      "51 22797\n",
      "52 23209\n",
      "53 23608\n",
      "54 24051\n",
      "55 24481\n",
      "56 24962\n",
      "57 25402\n",
      "58 25821\n",
      "59 26235\n",
      "60 26671\n",
      "61 27104\n",
      "62 27581\n",
      "63 28011\n",
      "64 28437\n",
      "65 28872\n",
      "66 29337\n",
      "67 29796\n",
      "68 30227\n",
      "69 30694\n",
      "70 31163\n",
      "71 31638\n",
      "72 32089\n",
      "73 32516\n",
      "74 32940\n",
      "75 33396\n",
      "76 33827\n",
      "77 34228\n",
      "78 34646\n",
      "79 35104\n",
      "80 35592\n",
      "81 36026\n",
      "82 36462\n",
      "83 36875\n",
      "84 37301\n",
      "85 37711\n",
      "86 38177\n",
      "87 38617\n",
      "88 39051\n",
      "89 39482\n",
      "90 39954\n",
      "91 40389\n",
      "92 40834\n",
      "93 41237\n",
      "94 41685\n",
      "95 42109\n",
      "96 42516\n",
      "97 42950\n",
      "98 43387\n",
      "99 43855\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "    \n",
    "import sys\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "repo_dir = '/srv/local/work/swang141/Sheng_repo/'\n",
    "sys.path.append(repo_dir)\n",
    "os.chdir(repo_dir)\n",
    "\n",
    "from pikachu.datasets.WordNet import parse_sentence\n",
    "import operator\n",
    "import collections\n",
    "from pikachu.models.text_classification.TextClassify import TextClassify\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "GO_term_dir = 'data/Pubmed/GO_sentences/'\n",
    "if not os.path.exists(GO_term_dir):\n",
    "    os.makedirs(GO_term_dir)\n",
    "    \n",
    "GO_term_file = 'data/NLP_Dictionary/go_term.txt'\n",
    "GO_term = set()\n",
    "\n",
    "fin = open(GO_term_file)\n",
    "for line in fin:\n",
    "    GO_term.add(line.lower().strip())\n",
    "fin.close()\n",
    "\n",
    "#GO_term = set()\n",
    "#GO_term.add('mll1 complex')\n",
    "#GO_term.add('binding')\n",
    "nchunk = 100\n",
    "pubmed_dir = '/srv/local/work/swang141/PatientSetAnnotation/Sheng/data/pubmed/'\n",
    "\n",
    "GO_term_ct = {}\n",
    "GO2text = {}\n",
    "\n",
    "sent_collection = []\n",
    "negative_set = []\n",
    "negative_thres = 0.999\n",
    "#print self.pubmed_dir\n",
    "for pid in range(nchunk):\n",
    "    fin = open(pubmed_dir+'pmid2meta_autophrase.chunk'+str(pid))\n",
    "    for line in fin:\n",
    "        text_l = line.lower().translate(None,',?!:()=%>/[]').strip().strip('.').split('. ')\n",
    "        #text_l = [line.lower().translate(None,',?!:()=%>/[]').strip().strip('.')]\n",
    "        for text in text_l:\n",
    "            pset = parse_sentence(GO_term,text,max_phrase_length = 5)\n",
    "            for p in pset:\n",
    "                if p not in GO2text:\n",
    "                    GO2text[p] = {}\n",
    "                GO2text[p][text] = 1\n",
    "                #print text\n",
    "            r = random.uniform(0, 1)\n",
    "            if r>negative_thres:\n",
    "                negative_set.append(text) \n",
    "    fin.close()\n",
    "    print pid,len(negative_set)\n",
    "negative_set = np.array(negative_set)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-26T03:05:27.065562Z",
     "start_time": "2018-06-26T03:05:26.888056Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pikachu.models.text_classification.TextClassify import TextClassify\n",
    "from sklearn import linear_model\n",
    "from pikachu.datasets.WordNet import parse_sentence\n",
    "import operator\n",
    "import collections\n",
    "from pikachu.models.text_classification.TextClassify import TextClassify\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "GO2keyword = {}\n",
    "nkeyword = 20\n",
    "\n",
    "e = list(GO2text['dna repair'])\n",
    "\n",
    "GO_predict_dir = 'data/Pubmed/predict_GO/'\n",
    "if not os.path.exists(GO_predict_dir):\n",
    "    os.makedirs(GO_predict_dir)\n",
    "nmax_train_data = 3000\n",
    "nmin_train_data = 3000\n",
    "for C in [10]:\n",
    "    for ct,GO in enumerate(GO_term):\n",
    "        if GO not in GO2text:\n",
    "            continue\n",
    "        if GO!= 'small molecular binding':\n",
    "            continue\n",
    "        GO_term_file = GO_term_dir + GO.replace('/','_').replace(' ','_') +'.pkl'\n",
    "        if len(GO2text[GO]) > nmax_train_data:\n",
    "            select_sent = np.random.choice(GO2text[GO].keys(), nmax_train_data, replace=False)\n",
    "            GO2text[GO] = {}\n",
    "            for t in select_sent:\n",
    "                GO2text[GO][t] = 1\n",
    "        nneg_data = min(max(nmin_train_data,len(GO2text[GO])*9), len(negative_set))\n",
    "\n",
    "        text_data = []\n",
    "        for t in GO2text[GO]:\n",
    "            text_data.append((t.replace(GO,''),1))    \n",
    "        text_data_neg = np.random.choice(negative_set, nneg_data, replace=False)\n",
    "\n",
    "        for text in text_data_neg:\n",
    "            if text not in GO2text[GO]:\n",
    "                text_data.append((text.replace('dna repair',''),0))\n",
    "        text_data = np.array(text_data)\n",
    "        np.random.shuffle(text_data)\n",
    "        train_data = text_data[:np.int(0.99*len(text_data))]\n",
    "        test_data = text_data[np.int(0.99*len(text_data)):]\n",
    "        has_pos = False\n",
    "        for t in train_data:\n",
    "            if t[1] == '1':\n",
    "                has_pos = True\n",
    "                break\n",
    "        if not has_pos:\n",
    "            continue\n",
    "        #lr = LogisticRegression(penalty='l1',C=1) #\n",
    "        lr = LogisticRegression()\n",
    "        text_clf = TextClassify()\n",
    "        text_clf.train(lr,train_data)\n",
    "        \n",
    "        \n",
    "        test_acc = text_clf.evaluate(test_data)\n",
    "        train_acc = text_clf.evaluate(train_data)\n",
    "        print GO,len(GO2text[GO]),test_acc,train_acc,text_clf.nword,len(np.nonzero(text_clf.clf.coef_)[1]),C\n",
    "        para = text_clf.clf.coef_[0]*-1\n",
    "        word_ind = np.argsort(para)[:20]\n",
    "        for i in range(20):\n",
    "            wi = word_ind[i]\n",
    "            print i,text_clf.i2w[wi],text_clf.feat_wt[wi],text_clf.clf.coef_[0][wi]\n",
    "        print '-------split line-------'\n",
    "        '''\n",
    "        with open(GO_term_file, 'wb') as output:\n",
    "            pickle.dump(text_clf, output, pickle.HIGHEST_PROTOCOL)\n",
    "        if ct%100==0:\n",
    "            print 'finished',ct*1.0/len(GO_term),ct,text_clf.nword\n",
    "\n",
    "        text_file = GO_predict_dir + GO.replace('/','_').replace(' ','_')+'.txt'\n",
    "        fout = open(text_file,'w')\n",
    "        text_clf = pickle.load(open(GO_term_file, \"rb\" ))\n",
    "        GO2keyword[GO] = text_clf.get_keyword(nkeyword)\n",
    "        print GO, len(GO2text[GO]),GO2keyword[GO]\n",
    "\n",
    "        nsent = 0\n",
    "        #print self.pubmed_dir\n",
    "        for pid in range(nchunk):\n",
    "            fin = open(pubmed_dir+'pmid2meta_autophrase.chunk'+str(pid))\n",
    "            for line in fin:\n",
    "                text_l = line.lower().translate(None,',?!:()=%>/[]').strip().strip('.').split('. ')\n",
    "                for text in text_l:\n",
    "                    pset = parse_sentence(GO2keyword[GO],text,max_phrase_length = 5)\n",
    "                    if len(pset) > 0 and GO not in text:\n",
    "                        lab = text_clf.predict(text)\n",
    "                        if lab==1:\n",
    "                            prob = text_clf.predict_prob(text)\n",
    "                            print prob,text\n",
    "                            fout.write(str(prob)+'\\t'+text+'\\n')\n",
    "                            nsent += 1\n",
    "            fin.close()\n",
    "            print pid, nsent, len(GO2text[GO])\n",
    "        fout.close()\n",
    "        print GO, nsent\n",
    "        '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-25T14:35:45.448417Z",
     "start_time": "2018-06-25T14:35:45.331124Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7007, 2)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T05:13:26.375683Z",
     "start_time": "2018-06-17T05:13:26.267599Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   13,    20,    33,    34,    49,    63,    85,    89,    98,\n",
       "          114,   143,   253,   419,   445,   460,   481,  1511,  2123,\n",
       "         2340,  3523,  5532,  5583,  9042, 10744]),)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(feat!=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T05:14:15.800299Z",
     "start_time": "2018-06-17T05:14:15.691080Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dna\n",
      "repair\n",
      "a\n"
     ]
    }
   ],
   "source": [
    "for i in np.nonzero(text_clf.clf.coef_)[1]:\n",
    "    print text_clf.i2w[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-09T12:33:40.158699Z",
     "start_time": "2018-07-09T12:32:59.731472Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set([])\n",
      "0 124 1\n",
      "1 124 1\n",
      "2 124 1\n",
      "3 124 1\n",
      "4 124 12\n",
      "5 124 12\n",
      "6 124 12\n",
      "7 124 12\n",
      "8 124 12\n",
      "9 124 12\n",
      "10 124 55\n",
      "11 124 59\n",
      "12 124 59\n",
      "13 124 59\n",
      "14 124 59\n",
      "15 124 99\n",
      "16 124 99\n",
      "17 124 99\n",
      "18 124 100\n",
      "19 124 101\n",
      "20 124 113\n",
      "21 124 113\n",
      "22 124 113\n",
      "23 124 113\n",
      "24 124 146\n",
      "25 124 146\n",
      "26 124 148\n",
      "27 124 148\n",
      "28 124 149\n",
      "29 124 159\n",
      "30 124 160\n",
      "31 124 167\n",
      "32 124 167\n",
      "33 124 167\n",
      "34 124 167\n",
      "35 124 167\n",
      "36 124 167\n",
      "37 124 167\n",
      "38 124 429\n",
      "39 124 429\n",
      "40 124 429\n",
      "41 124 429\n",
      "42 124 3326\n",
      "43 124 3331\n",
      "44 124 3332\n",
      "45 124 3332\n",
      "46 124 3355\n",
      "47 124 3355\n",
      "48 124 3358\n",
      "49 124 3358\n",
      "50 124 3358\n",
      "51 124 3358\n",
      "52 124 3358\n",
      "53 124 3358\n",
      "54 124 3358\n",
      "55 124 3358\n",
      "56 124 3358\n",
      "57 124 3370\n",
      "58 124 3370\n",
      "59 124 3370\n",
      "60 124 3372\n",
      "61 124 3384\n",
      "62 124 3385\n",
      "63 124 3385\n",
      "64 124 3385\n",
      "65 124 3385\n",
      "66 124 3385\n",
      "67 124 3385\n",
      "68 124 3385\n",
      "69 124 3385\n",
      "70 124 3385\n",
      "71 124 3385\n",
      "72 124 3411\n",
      "73 124 3411\n",
      "74 124 3412\n",
      "75 124 3461\n",
      "76 124 3461\n",
      "77 124 3464\n",
      "78 124 3476\n",
      "79 124 3476\n",
      "80 124 3476\n",
      "81 124 3478\n",
      "82 124 3478\n",
      "83 124 3478\n",
      "84 124 3478\n",
      "85 124 3478\n",
      "86 124 3478\n",
      "87 124 3478\n",
      "88 124 3478\n",
      "89 124 3478\n",
      "90 124 3478\n",
      "91 124 3478\n",
      "92 124 3478\n",
      "93 124 3478\n",
      "94 124 3584\n",
      "95 124 3584\n",
      "96 124 3585\n",
      "97 124 3585\n",
      "98 124 3585\n",
      "99 124 3589\n",
      "100 124 3589\n",
      "101 124 3655\n",
      "102 124 3655\n",
      "103 124 3655\n",
      "104 124 3655\n",
      "105 124 3825\n",
      "106 124 3825\n",
      "107 124 3825\n",
      "108 124 3825\n",
      "109 124 3826\n",
      "110 124 3826\n",
      "111 124 3826\n",
      "112 124 3827\n",
      "113 124 3827\n",
      "114 124 3827\n",
      "115 124 3827\n",
      "116 124 3827\n",
      "117 124 3829\n",
      "118 124 3829\n",
      "119 124 3829\n",
      "120 124 3829\n",
      "121 124 3829\n",
      "122 124 3829\n",
      "123 124 3829\n",
      "read data finished number of negatives 3829\n",
      "aryl sulfotransferase activity\n",
      "aryl sulfotransferase activity 5\n",
      "aryl sulfotransferase activity train\n",
      "aryl sulfotransferase activity 5 positive samples\n",
      "aryl sulfotransferase activity 3005 all samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pikachu/models/text_classification/TextClassify.py:188: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  self.feat_wt[i] = (np.sum(trainX[pos,i]) * 1.0 / len(pos) ) /(np.sum(trainX[neg,i]) * 1.0 / len(neg) )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "platelet morphogenesis\n",
      "platelet morphogenesis 3\n",
      "platelet morphogenesis train\n",
      "platelet morphogenesis 3 positive samples\n",
      "platelet morphogenesis 3003 all samples\n",
      "chromatoid body\n",
      "chromatoid body 122\n",
      "chromatoid body train\n",
      "chromatoid body 122 positive samples\n",
      "chromatoid body 3122 all samples\n",
      "6-phospho-beta-galactosidase activity\n",
      "6-phospho-beta-galactosidase activity 1\n",
      "6-phospho-beta-galactosidase activity train\n",
      "6-phospho-beta-galactosidase activity 1 positive samples\n",
      "6-phospho-beta-galactosidase activity 3001 all samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/local/work/swang141/software/Anaconda/anaconda/lib/python2.7/site-packages/sklearn/metrics/ranking.py:547: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "endonuclease activity\n",
      "endonuclease activity 1344\n",
      "endonuclease activity train\n",
      "endonuclease activity 1344 positive samples\n",
      "endonuclease activity 4344 all samples\n",
      "oxidative stress-induced premature senescence\n",
      "oxidative stress-induced premature senescence 15\n",
      "oxidative stress-induced premature senescence train\n",
      "oxidative stress-induced premature senescence 15 positive samples\n",
      "oxidative stress-induced premature senescence 3015 all samples\n",
      "histone h3-k79 methylation\n",
      "histone h3-k79 methylation 1\n",
      "histone h3-k79 methylation train\n",
      "histone h3-k79 methylation 1 positive samples\n",
      "histone h3-k79 methylation 3001 all samples\n",
      "immature t cell proliferation\n",
      "immature t cell proliferation 1\n",
      "immature t cell proliferation train\n",
      "immature t cell proliferation 1 positive samples\n",
      "immature t cell proliferation 3001 all samples\n",
      "carboxy-lyase activity\n",
      "carboxy-lyase activity 5\n",
      "carboxy-lyase activity train\n",
      "carboxy-lyase activity 5 positive samples\n",
      "carboxy-lyase activity 3005 all samples\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d4d4a176caa7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m'read data finished number of negatives'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnegative_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m     \u001b[0mcall_merge_preprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_GO_term\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mGO_text_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mGO_pred_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mGO_sent_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnegative_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0mpred_GO_term\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-d4d4a176caa7>\u001b[0m in \u001b[0;36mcall_merge_preprocess\u001b[0;34m(GO_term_set, GO_text_dir, GO_pred_dir, GO_sent_dir, negative_set)\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0;31m#try:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m             \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGO\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mGO2text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtext_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlog_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m             \u001b[0;31m#except Exception, e:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0;31m#    continue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-d4d4a176caa7>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(GO, GO2text, text_file, pred_file, log_file, nfold)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_pos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0mtext_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv_train_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv_test_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv_train_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0mfout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/local/work/swang141/Sheng_repo/pikachu/models/text_classification/TextClassify.pyc\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, test_text_data)\u001b[0m\n\u001b[1;32m    258\u001b[0m                 \u001b[0mprob_l\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_text_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m                         \u001b[0mpred_l\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m                         \u001b[0mprob_l\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m                         \u001b[0mtruth_l\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/local/work/swang141/Sheng_repo/pikachu/models/text_classification/TextClassify.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, sent)\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 \u001b[0;31m# train or predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m                 \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/local/work/swang141/Sheng_repo/pikachu/models/text_classification/TextClassify.pyc\u001b[0m in \u001b[0;36mget_feature\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    238\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw2i\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m                                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m                         \u001b[0mexample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw2i\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "import sys\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import operator\n",
    "import collections\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "def write_GO_to_file(pid, GO_sent_dir, GO2text):\n",
    "    cal_feat_time = 0.\n",
    "    pickle_time = 0.\n",
    "    cal_prob_time = 0.\n",
    "    max_abs = 50\n",
    "    for ct,GO in enumerate(GO2text):\n",
    "        sent2prob = {}\n",
    "        sent_file = GO_sent_dir + str(pid)+'/'+ GO.replace('/','_').replace(' ','_')+'.txt'\n",
    "\n",
    "        pred_file =    GO_pred_dir + GO.replace('/','_').replace(' ','_')+'.pkl'\n",
    "        start = time.time()\n",
    "        text_clf = pickle.load(open(pred_file, \"rb\" ))\n",
    "        end = time.time()\n",
    "        pickle_time += end - start\n",
    "        text2prob = {}\n",
    "        for text in GO2text[GO]:\n",
    "            '''\n",
    "            start = time.time()\n",
    "            feat = [text_clf.get_test_feature(text)]\n",
    "            end = time.time()\n",
    "            cal_feat_time += end - start\n",
    "            start = time.time()\n",
    "            #prob = text_clf.predict_prob(text)\n",
    "            #prob = text_clf.predict_prob_feat(feat)\n",
    "            feat_pos = np.nonzero(text_clf.clf.coef_)[1]\n",
    "            sc = text_clf.clf.intercept_[0]\n",
    "            for f in feat_pos:\n",
    "                sc += text_clf.clf.coef_[0][f] * feat[0][f]\n",
    "            prob = 1 / (1 + np.exp(-1*sc))\n",
    "            prob1 = text_clf.predict_prob_feat(feat)\n",
    "            print prob,prob1\n",
    "            end = time.time()\n",
    "            cal_prob_time += end - start\n",
    "            '''\n",
    "            prob = text_clf.predict_prob_fast(text)\n",
    "            if prob > 0.99:\n",
    "                text2prob[text] = prob*-1\n",
    "                #fout.write(text+'\\t'+str(prob)+'\\n')\n",
    "        fout = open(sent_file,'w')\n",
    "        text2prob_sort = sorted(text2prob.items(), key = lambda item:item[1])\n",
    "        noutput = min(max_abs, len(text2prob_sort))\n",
    "        for i in range(noutput):\n",
    "            fout.write(text2prob_sort[i][0]+'\\t' + str(text2prob_sort[i][1])+'\\n')\n",
    "        fout.close()\n",
    "        if ct%100==0:\n",
    "            print 'sent2prob',ct*1.0/len(GO2text)\n",
    "            sys.stdout.flush()\n",
    "\n",
    "\n",
    "def predict_GO_term(proc_id, keyword2GO,p2chunk,GO_sent_dir,GO_pred_dir):\n",
    "    keyword_set = set(keyword2GO.keys())\n",
    "    print len(keyword_set)\n",
    "    parse_time = 0.\n",
    "    append_time = 0.\n",
    "    nsent = 0\n",
    "    GO2text = {}\n",
    "    #print self.pubmed_dir\n",
    "    for pid in p2chunk:\n",
    "        fin = open(pubmed_dir+'pmid2meta_autophrase.chunk'+str(pid))\n",
    "        for ct,line in enumerate(fin):\n",
    "            text_l = [line.lower().translate(None,',?!:()=%>/[]').strip().strip('.')]\n",
    "            for text in text_l:\n",
    "                start = time.time()\n",
    "                pset = parse_sentence(keyword_set,text,max_phrase_length = 1)\n",
    "                end = time.time()\n",
    "                parse_time += end - start\n",
    "                start = time.time()\n",
    "                cur_GO = set()\n",
    "                for p in pset:\n",
    "                    for GO in keyword2GO[p]:\n",
    "                        if GO in text:\n",
    "                            continue\n",
    "                        if GO in cur_GO:\n",
    "                            continue\n",
    "                        cur_GO.add(GO)\n",
    "                        if GO not in GO2text:\n",
    "                            GO2text[GO] = []\n",
    "                        GO2text[GO].append(text)\n",
    "                        #fout.write(str(prob)+'\\t'+text+'\\n')\n",
    "                        nsent += 1\n",
    "                end = time.time()\n",
    "                append_time += end - start\n",
    "            if ct%1000==0:\n",
    "                print proc_id, pid, 'read finished',ct/40000.\n",
    "                sys.stdout.flush()\n",
    "        print proc_id, pid,'start to write file',parse_time,append_time\n",
    "        sys.stdout.flush()\n",
    "        write_GO_to_file(proc_id,GO_sent_dir,GO2text)\n",
    "        GO2text = {}\n",
    "        print proc_id, pid,'end to write file'\n",
    "        sys.stdout.flush()\n",
    "        fin.close()\n",
    "\n",
    "def generate_train_model(list, n):\n",
    "    N = len(list)\n",
    "    ind = np.random.choice(N, n)\n",
    "    new_list = []\n",
    "    for i in ind:\n",
    "        new_list.append(list[i])\n",
    "    return new_list\n",
    "    #for i in range(n):\n",
    "    #    yield np.random.choice(list, 1)\n",
    "\n",
    "def train_model(GO, GO2text, text_file, pred_file, log_file, nfold = 3):\n",
    "    print GO,'train'\n",
    "    sys.stdout.flush()\n",
    "    GO2keyword = {}\n",
    "    nmax_train_data = 3000\n",
    "    nmin_train_data = 3000\n",
    "\n",
    "    train_data = []\n",
    "    if len(GO2text) > nmax_train_data:\n",
    "        select_sent = generate_train_model(GO2text.keys(), nmax_train_data)\n",
    "        for t in select_sent:\n",
    "            train_data.append((t[0].replace(GO,''),1))\n",
    "    else:\n",
    "        for t in GO2text:\n",
    "            train_data.append((t.replace(GO,''),1))\n",
    "    sys.stdout.flush()\n",
    "    ntrain_data = min(nmin_train_data, len(negative_set))\n",
    "    train_data_neg = generate_train_model(negative_set, ntrain_data)\n",
    "    print GO,len(train_data),'positive samples'\n",
    "    sys.stdout.flush()\n",
    "    for text in train_data_neg:\n",
    "        if text[0] not in GO2text:\n",
    "            train_data.append((text[0],0))\n",
    "    print GO,len(train_data),'all samples'\n",
    "    sys.stdout.flush()\n",
    "    lr = LogisticRegression()#penalty='l1',C=0.1\n",
    "    text_clf = TextClassify()\n",
    "    \n",
    "    train_data = np.array(train_data)\n",
    "    cv_data = train_data\n",
    "    np.random.shuffle(cv_data)\n",
    "    ratio = 1 - 1.0 / nfold\n",
    "    cv_train_data = cv_data[:np.int(ratio*len(cv_data))]\n",
    "    cv_test_data = cv_data[np.int(ratio*len(cv_data)):]\n",
    "    fout = open(log_file,'w')\n",
    "    has_pos = False\n",
    "    for t in cv_train_data:\n",
    "        if t[1] == '1':\n",
    "            has_pos = True\n",
    "            break    \n",
    "    if has_pos:\n",
    "        text_clf.train(lr,cv_train_data)\n",
    "        test_acc = text_clf.evaluate(cv_test_data)\n",
    "        train_acc = text_clf.evaluate(cv_train_data)        \n",
    "        fout.write(str(test_acc)+'\\t'+str(train_acc)+'\\t'+str(text_clf.nword)+'\\n')\n",
    "        para = text_clf.clf.coef_[0]\n",
    "        word_ind = np.argsort(para)[:20]\n",
    "        for i in range(20):\n",
    "            wi = word_ind[i]\n",
    "            fout.write(str(i)+'\\t'+str(text_clf.i2w[wi])+'\\t'+str(text_clf.clf.coef_[0][wi])+'\\n')\n",
    "    #lr = LogisticRegression(penalty='l1',C=1) #    \n",
    "    #print test_acc, train_acc,text_clf.nword    \n",
    "    text_clf.train(lr,train_data)\n",
    "    para = text_clf.clf.coef_[0]*-1\n",
    "    word_ind = np.argsort(para)[:20]\n",
    "    for i in range(20):\n",
    "        wi = word_ind[i]\n",
    "        fout.write(str(i)+'\\t'+str(text_clf.i2w[wi])+'\\t'+str(text_clf.clf.coef_[0][wi])+'\\n')\n",
    "    sys.stdout.flush()\n",
    "    fout.close()\n",
    "    with open(pred_file, 'wb') as output:\n",
    "        pickle.dump(text_clf, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def call_merge_preprocess(GO_term_set,GO_text_dir,GO_pred_dir,GO_sent_dir,negative_set):\n",
    "    for ct, GO in enumerate(GO_term_set):\n",
    "        text_file = GO_text_dir + GO.replace('/','_').replace(' ','_')+'.pkl'\n",
    "        if not os.path.isfile(text_file):\n",
    "            continue\n",
    "        print GO\n",
    "        sys.stdout.flush()\n",
    "        GO2text = pickle.load(open(text_file, \"rb\" ))\n",
    "        pred_file = GO_pred_dir + GO.replace('/','_').replace(' ','_')+'.pkl'\n",
    "        log_file = GO_pred_dir + GO.replace('/','_').replace(' ','_')+'.log'\n",
    "        if not os.path.isfile(pred_file):\n",
    "            print GO,len(GO2text)\n",
    "            sys.stdout.flush()\n",
    "            #try:\n",
    "            train_model(GO,GO2text,text_file,pred_file,log_file)\n",
    "            #except Exception, e:\n",
    "            #    continue\n",
    "        if not os.path.isfile(pred_file):\n",
    "            continue\n",
    "        text_clf = pickle.load(open(pred_file, \"rb\" ))\n",
    "\n",
    "def call_predict_preprocess(i,npid,p2chunk,GO_term_set,GO_pred_dir,GO_sent_dir):\n",
    "    start = time.time()\n",
    "    keyword2GO = {}\n",
    "    nkeyword = 20\n",
    "    ct = 0\n",
    "    print len(GO_term_set)\n",
    "    sys.stdout.flush()\n",
    "    for ct, GO in enumerate(GO_term_set):\n",
    "        pred_file = GO_pred_dir + GO.replace('/','_').replace(' ','_')+'.pkl'\n",
    "        if not os.path.isfile(pred_file):\n",
    "            continue\n",
    "        text_clf = pickle.load(open(pred_file, \"rb\" ))\n",
    "        key_word = text_clf.get_keyword(nkeyword)\n",
    "        print GO,key_word\n",
    "        sys.stdout.flush()\n",
    "        for w in key_word:\n",
    "            if w not in keyword2GO:\n",
    "                keyword2GO[w] = []\n",
    "            keyword2GO[w].append(GO)\n",
    "            ct += 1\n",
    "        if ct%100==0:\n",
    "            end = time.time()\n",
    "            print 'len keyword',ct,end-start\n",
    "            sys.stdout.flush()\n",
    "    end = time.time()\n",
    "    print 'len keyword',ct,end-start\n",
    "    sys.stdout.flush()\n",
    "    predict_GO_term(i, keyword2GO,p2chunk,GO_sent_dir,GO_pred_dir)\n",
    "\n",
    "proc_id = 1\n",
    "totalpid = 64\n",
    "nchunk = 400\n",
    "mode = 1#0: extract abs, 1: train_model, 2:generate sent\n",
    "server = 'timan107'#'grenache'\n",
    "if server == 'timan107':\n",
    "    pubmed_dir = '/srv/local/work/swang141/PatientSetAnnotation/Sheng/data/pubmed/'\n",
    "    GO_data_dir = '/srv/local/work/swang141/Sheng_repo/data/Pubmed/'\n",
    "    GO_term_file = '/srv/local/work/swang141/Sheng_repo/data/NLP_Dictionary/go_term.txt'\n",
    "    repo_dir = '/srv/local/work/swang141/Sheng_repo/'\n",
    "else:\n",
    "    pubmed_dir = '/data/cellardata/users/majianzhu/wangsheng/Sheng_repo/pubmed/'\n",
    "    GO_data_dir = '/cellar/users/majianzhu/Data/wangsheng/Sheng_repo/data/Pubmed/'\n",
    "    GO_term_file = '/data/cellardata/users/majianzhu/wangsheng/Sheng_repo/Sheng_repo/data/NLP_Dictionary/go_term.txt'\n",
    "    repo_dir = '/data/cellardata/users/majianzhu/wangsheng/Sheng_repo/Sheng_repo/'\n",
    "\n",
    "\n",
    "\n",
    "sys.path.append(repo_dir)\n",
    "os.chdir(repo_dir)\n",
    "\n",
    "from pikachu.datasets.WordNet import parse_sentence\n",
    "from pikachu.models.text_classification.TextClassify import TextClassify\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "GO_text_dir = GO_data_dir+'GO_abstracts_test/'\n",
    "GO_pred_dir = GO_data_dir + 'GO_models_L2/'\n",
    "GO_sent_dir = GO_data_dir + 'GO_sentences_test/'\n",
    "GO_sent_merge_dir = GO_data_dir + 'GO_sentences_all/'\n",
    "\n",
    "if not os.path.exists(GO_sent_merge_dir):\n",
    "    os.makedirs(GO_sent_merge_dir)\n",
    "if not os.path.exists(GO_sent_dir):\n",
    "    os.makedirs(GO_sent_dir)\n",
    "if not os.path.exists(GO_pred_dir):\n",
    "    os.makedirs(GO_pred_dir)\n",
    "if not os.path.exists(GO_text_dir):\n",
    "    os.makedirs(GO_text_dir)\n",
    "\n",
    "\n",
    "for i in range(totalpid):\n",
    "    GO_sent_dir_pid = GO_sent_dir +str(i)+'/'\n",
    "    if not os.path.exists(GO_sent_dir_pid):\n",
    "        os.makedirs(GO_sent_dir_pid)\n",
    "\n",
    "\n",
    "pred_GO_term = set()\n",
    "abs_GO_term = set()\n",
    "sent_GO_term = set()\n",
    "fin = open(GO_term_file)\n",
    "ct = 0\n",
    "for line in fin:\n",
    "    GO = line.lower().strip()\n",
    "    text_file = GO_text_dir + GO.replace('/','_').replace(' ','_')+'.pkl'\n",
    "    pred_file = GO_pred_dir + GO.replace('/','_').replace(' ','_')+'.pkl'\n",
    "    sent_file = GO_sent_dir + GO.replace('/','_').replace(' ','_')+'.txt'\n",
    "    if mode == 1 and not os.path.isfile(pred_file) and os.path.isfile(text_file):\n",
    "        ct+=1\n",
    "        if ct%totalpid == proc_id:\n",
    "            pred_GO_term.add(GO)\n",
    "    if mode == 0 and not os.path.isfile(text_file):\n",
    "        ct+=1\n",
    "        if ct%totalpid == proc_id:\n",
    "            abs_GO_term.add(GO)\n",
    "    if mode == 2 and not os.path.isfile(sent_file) and os.path.isfile(pred_file):\n",
    "        ct+=1\n",
    "        if ct%totalpid == proc_id:\n",
    "            sent_GO_term.add(GO)\n",
    "fin.close()\n",
    "GO_term_ct = {}\n",
    "GO2text = {}\n",
    "print sent_GO_term\n",
    "\n",
    "if mode==1 or mode == 0:\n",
    "    sent_collection = []\n",
    "    negative_set = []\n",
    "    negative_thres = 0.99\n",
    "    if len(abs_GO_term) > 0:\n",
    "        for pid in range(nchunk):\n",
    "            fin = open(pubmed_dir+'pmid2meta_autophrase.chunk'+str(pid))\n",
    "            for line in fin:\n",
    "                text_l = [line.lower().translate(None,',?!:()=%>/[]').strip().strip('.')]\n",
    "                for text in text_l:\n",
    "                    pset = parse_sentence(abs_GO_term,text,max_phrase_length = 5)\n",
    "                    for p in pset:\n",
    "                        if p not in GO2text:\n",
    "                            GO2text[p] = {}\n",
    "                        GO2text[p][text] = 1\n",
    "            fin.close()\n",
    "            print pid,len(negative_set)\n",
    "            sys.stdout.flush()\n",
    "        \n",
    "\n",
    "    for ct, GO in enumerate(GO2text):\n",
    "        GO2text_file = GO_text_dir+GO.replace('/','_').replace(' ','_')+'.pkl'\n",
    "        if os.path.isfile(GO2text_file):\n",
    "            continue\n",
    "        with open(GO2text_file, 'wb') as output:\n",
    "            pickle.dump(GO2text[GO], output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    for ct, GO in enumerate(pred_GO_term):\n",
    "        text_file = GO_text_dir + GO.replace('/','_').replace(' ','_')+'.pkl'\n",
    "        if not os.path.isfile(text_file):\n",
    "            continue\n",
    "        GO2text = pickle.load(open(text_file, \"rb\" ))\n",
    "        for text in GO2text:\n",
    "            r = random.uniform(0, 1)\n",
    "            if r>negative_thres:\n",
    "                negative_set.append(text.replace(GO,''))\n",
    "        print ct, len(pred_GO_term), len(negative_set)\n",
    "        \n",
    "    negative_set = np.array(negative_set)\n",
    "    print 'read data finished number of negatives',len(negative_set)\n",
    "    sys.stdout.flush()\n",
    "    call_merge_preprocess(pred_GO_term,GO_text_dir,GO_pred_dir,GO_sent_dir,negative_set)\n",
    "elif mode==2:\n",
    "    pred_GO_term = set()\n",
    "    abs_GO_term = set()\n",
    "    sent_GO_term = set()\n",
    "    fin = open(GO_term_file)\n",
    "    ct = 0\n",
    "    for line in fin:\n",
    "        GO = line.lower().strip()\n",
    "        sent_file = GO_sent_dir + str(proc_id)+'/'+ GO.replace('/','_').replace(' ','_')+'.txt'\n",
    "        #if os.path.isfile(sent_file):\n",
    "        #    continue\n",
    "        sent_GO_term.add(GO)\n",
    "        ct += 1\n",
    "    print 'ngo',len(sent_GO_term)\n",
    "    sys.stdout.flush()\n",
    "    nchunk = 400\n",
    "    p2chunk = {}\n",
    "    for i in range(nchunk):\n",
    "        pid = i%totalpid\n",
    "        if pid not in p2chunk:\n",
    "            p2chunk[pid] = set()\n",
    "        p2chunk[pid].add(i)\n",
    "    call_predict_preprocess(proc_id,totalpid,p2chunk[proc_id-1],sent_GO_term,GO_pred_dir,GO_sent_dir)\n",
    "elif mode==3:\n",
    "    sent_GO_term = set()\n",
    "    fin = open(GO_term_file)\n",
    "    ct = 0\n",
    "    for line in fin:\n",
    "        GO = line.lower().strip()\n",
    "        sent_GO_term.add(GO)\n",
    "        ct += 1\n",
    "    for ct,GO in enumerate(sent_GO_term):\n",
    "        print GO,ct\n",
    "        new_sent_file = GO_sent_merge_dir +  GO.replace('/','_').replace(' ','_')+'.txt'\n",
    "        fout = open(new_sent_file,'w')\n",
    "        for i in range(totalpid):\n",
    "            sent_file = GO_sent_dir + str(i)+'/'+ GO.replace('/','_').replace(' ','_')+'.txt'\n",
    "            if not os.path.isfile(sent_file):\n",
    "                continue\n",
    "            fin = open(sent_file)\n",
    "            for line in fin:\n",
    "                fout.write(line.strip()+'\\n')\n",
    "            fin.close()\n",
    "        fout.close()\n",
    "else:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T04:26:50.482055Z",
     "start_time": "2018-06-17T04:26:50.367094Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9597,     4,  8534,    37,  2355, 11398,  6411,  2231, 12411,\n",
       "       12410,     5,  7849,  7848,  1788,  1487,  1563,  4381,    65,\n",
       "       11926,  1101])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-09T12:39:45.171661Z",
     "start_time": "2018-07-09T12:39:44.075890Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oxidative stress-induced premature senescence\n",
      "oxidative stress-induced premature senescence 15\n",
      "oxidative stress-induced premature senescence train\n",
      "oxidative stress-induced premature senescence 15 positive samples\n",
      "oxidative stress-induced premature senescence 3015 all samples\n"
     ]
    }
   ],
   "source": [
    "def train_model(GO, GO2text, text_file, pred_file, log_file, nfold = 3):\n",
    "    print GO,'train'\n",
    "    sys.stdout.flush()\n",
    "    GO2keyword = {}\n",
    "    nmax_train_data = 3000\n",
    "    nmin_train_data = 3000\n",
    "\n",
    "    train_data = []\n",
    "    if len(GO2text) > nmax_train_data:\n",
    "        select_sent = generate_train_model(GO2text.keys(), nmax_train_data)\n",
    "        for t in select_sent:\n",
    "            train_data.append((t[0].replace(GO,''),1))\n",
    "    else:\n",
    "        for t in GO2text:\n",
    "            train_data.append((t.replace(GO,''),1))\n",
    "    sys.stdout.flush()\n",
    "    ntrain_data = min(nmin_train_data, len(negative_set))\n",
    "    train_data_neg = generate_train_model(negative_set, ntrain_data)\n",
    "    print GO,len(train_data),'positive samples'\n",
    "    sys.stdout.flush()\n",
    "    for text in train_data_neg:\n",
    "        if text[0] not in GO2text:\n",
    "            train_data.append((text[0],0))\n",
    "    print GO,len(train_data),'all samples'\n",
    "    sys.stdout.flush()\n",
    "    lr = LogisticRegression()#penalty='l1',C=0.1\n",
    "    text_clf = TextClassify()\n",
    "    \n",
    "    train_data = np.array(train_data)\n",
    "    cv_data = train_data\n",
    "    np.random.shuffle(cv_data)\n",
    "    ratio = 1 - 1.0 / nfold\n",
    "    cv_train_data = cv_data[:np.int(ratio*len(cv_data))]\n",
    "    cv_test_data = cv_data[np.int(ratio*len(cv_data)):]\n",
    "    fout = open(log_file,'w')\n",
    "    has_pos = False\n",
    "    for t in cv_train_data:\n",
    "        if t[1] == '1':\n",
    "            has_pos = True\n",
    "            break    \n",
    "    if has_pos:\n",
    "        text_clf.train(lr,cv_train_data)\n",
    "        test_acc = text_clf.evaluate(cv_test_data)\n",
    "        train_acc = text_clf.evaluate(cv_train_data)        \n",
    "        fout.write(str(test_acc)+'\\t'+str(train_acc)+'\\t'+str(text_clf.nword)+'\\n')\n",
    "        para = text_clf.clf.coef_[0]\n",
    "        word_ind = np.argsort(para)[:20]\n",
    "        for i in range(20):\n",
    "            wi = word_ind[i]\n",
    "            fout.write(str(i)+'\\t'+str(text_clf.i2w[wi])+'\\t'+str(text_clf.clf.coef_[0][wi])+'\\n')\n",
    "    #lr = LogisticRegression(penalty='l1',C=1) #    \n",
    "    #print test_acc, train_acc,text_clf.nword    \n",
    "    text_clf.train(lr,train_data)\n",
    "    para = text_clf.clf.coef_[0]*-1\n",
    "    word_ind = np.argsort(para)[:20]\n",
    "    for i in range(20):\n",
    "        wi = word_ind[i]\n",
    "        fout.write(str(i)+'\\t'+str(text_clf.i2w[wi])+'\\t'+str(text_clf.clf.coef_[0][wi])+'\\n')\n",
    "    sys.stdout.flush()\n",
    "    fout.close()\n",
    "    with open(pred_file, 'wb') as output:\n",
    "        pickle.dump(text_clf, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "\n",
    "call_merge_preprocess(['oxidative stress-induced premature senescence'],GO_text_dir,GO_pred_dir,GO_sent_dir,negative_set)\n",
    "\n",
    "for ct, GO in enumerate(['oxidative stress-induced premature senescence']):\n",
    "    text_file = GO_text_dir + GO.replace('/','_').replace(' ','_')+'.pkl'\n",
    "    if not os.path.isfile(text_file):\n",
    "        continue\n",
    "    print GO\n",
    "    sys.stdout.flush()\n",
    "    GO2text = pickle.load(open(text_file, \"rb\" ))\n",
    "    pred_file = GO_pred_dir + GO.replace('/','_').replace(' ','_')+'.pkl'\n",
    "    log_file = GO_pred_dir + GO.replace('/','_').replace(' ','_')+'.log'\n",
    "    if not os.path.isfile(pred_file):\n",
    "        print GO,len(GO2text)\n",
    "        sys.stdout.flush()\n",
    "        #try:\n",
    "        train_model(GO,GO2text,text_file,pred_file,log_file)\n",
    "        #except Exception, e:\n",
    "        #    continue\n",
    "    if not os.path.isfile(pred_file):\n",
    "        continue\n",
    "    text_clf = pickle.load(open(pred_file, \"rb\" ))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-09T12:38:40.503190Z",
     "start_time": "2018-07-09T12:38:40.476862Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/srv/local/work/swang141/Sheng_repo/data/Pubmed/GO_models_L2/oxidative_stress-induced_premature_senescence.pkl'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-15T22:33:10.813888Z",
     "start_time": "2018-06-15T22:33:10.810657Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3193135828470574"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " np.exp(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-15T22:33:38.398741Z",
     "start_time": "2018-06-15T22:33:38.395458Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.12873856858812432"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.intercept_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
