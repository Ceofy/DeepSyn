{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T19:55:29.790328Z",
     "start_time": "2018-03-01T19:55:25.812258Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ortools.graph import pywrapgraph\n",
    "import os\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import copy\n",
    "import operator\n",
    "from graphviz import Digraph\n",
    "from IPython.display import IFrame\n",
    "from nltk.stem.porter import *\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "#tg = 'zinc finger'\n",
    "def query_path(tg,d):\n",
    "    #tg = 'zmym2'\n",
    "    #d='lung adenocarcinoma'\n",
    "    pre = {}\n",
    "    visited = set()\n",
    "    visited.add(tg)\n",
    "    net = ect\n",
    "    find = False\n",
    "    all_node = set()\n",
    "    all_node.add(tg)\n",
    "    for iter in range(100):\n",
    "        new_node = set()\n",
    "        for ss in visited:\n",
    "            if ss == d:\n",
    "                tmp = ss\n",
    "                while tmp in pre:\n",
    "                    print tmp,node_type[tmp],'->',pre[tmp]\n",
    "                    tmp = pre[tmp]\n",
    "                find = True\n",
    "                break\n",
    "            if ss not in net:\n",
    "                continue\n",
    "            for e in net[ss]:\n",
    "                if e in all_node:\n",
    "                    continue\n",
    "                new_node.add(e) \n",
    "                all_node.add(e)\n",
    "                pre[e] = ss\n",
    "        visited = new_node     \n",
    "        if find:\n",
    "            break\n",
    "            \n",
    "def plot_figure(node_list, edge_list,output_file,net_edge_weight):\n",
    "    dot = Digraph(comment='Network flow')\n",
    "    for node in node_list:\n",
    "        if node==source_node:\n",
    "            dot.attr('node',color='blue')\n",
    "            dot.node(node)\n",
    "        elif node in d2g[d]:\n",
    "            dot.attr('node',color='red')\n",
    "            dot.node(node) \n",
    "        elif node in node_type and node_type[node] == 'function':\n",
    "            dot.attr('node',color='green')\n",
    "            dot.node(node)          \n",
    "        elif node in node_type and node_type[node] == 'symptom':\n",
    "            dot.attr('node',color='yellow')\n",
    "            dot.node(node)  \n",
    "        else:        \n",
    "            dot.attr('node',color='black')\n",
    "            dot.node(node)   \n",
    "    for e in edge_list:        \n",
    "        e1,e2,w,mw = e\n",
    "        if e1 in kg_relation and e2 in kg_relation[e1]:\n",
    "            label = 'Percha et al.'\n",
    "        else:\n",
    "            label = 'text mining'\n",
    "        if e1 not in node_type:\n",
    "            t1 = 'entity'\n",
    "        else:\n",
    "            t1 = node_type[e1]\n",
    "        if e2 not in node_type:\n",
    "            t2 = 'entity'\n",
    "        else:\n",
    "            t2 = node_type[e2]\n",
    "        if e1 not in wct:\n",
    "            wct[e1] = 0\n",
    "        if e2 not in wct:\n",
    "            wct[e2] = 0\n",
    "        edge_weight = 0\n",
    "        if e1 in net_edge_weight and e2 in net_edge_weight[e1]:\n",
    "            edge_weight = net_edge_weight[e1][e2]\n",
    "        dot.edge(e1,e2,label=label+'('+t1+'-'+t2+':'+str(w)+')'+str(edge_weight))\n",
    "    dot.render(output_file) \n",
    "    \n",
    "def dfs(cur_node,target_node,net_flow_edge,node_type,edge_type,max_depth,cur_depth,path,global_all_path):\n",
    "    #print cur_node,target_node,cur_depth,max_depth,net_flow_edge[cur_node]\n",
    "    if cur_node == target_node:\n",
    "        #print path\n",
    "        global_all_path.append(path)\n",
    "        return global_all_path\n",
    "    if cur_depth == max_depth:\n",
    "        #print path\n",
    "        return []\n",
    "    #print cur_depth\n",
    "    argu_path = []\n",
    "    for ngh in net_flow_edge[cur_node]:\n",
    "        if True:#[node_type[cur_node],node_type[ngh]] in edge_type:\n",
    "            #print ngh,path\n",
    "            new_path = dfs(ngh,target_node,net_flow_edge,node_type,edge_type,max_depth,cur_depth+1,path+'\\t'+ngh,global_all_path)\n",
    "            #if len(new_path) > 0:\n",
    "            #    argu_path.append(new_path)\n",
    "    return global_all_path\n",
    "\n",
    "def dfs_find_graph(edge_type,source_node,target_node,select_node_set,net_flow_edge,node2i,i2node,node_type,max_depth = 5):    \n",
    "    visited = set()\n",
    "    all_path = dfs(source_node,target_node,net_flow_edge,node_type,edge_type,max_depth,0,source_node,[])\n",
    "    npath = len(all_path)\n",
    "    edge_d = {}\n",
    "    node_list = set()\n",
    "    #print npath\n",
    "    #print all_path\n",
    "    for p in range(npath):\n",
    "        path = all_path[p].split('\\t')\n",
    "        for w in range(0,len(path)-1):\n",
    "            node_list.add(path[w])\n",
    "            node_list.add(path[w+1])\n",
    "            edge_d[(path[w],path[w+1])] = edge_d.get((path[w],path[w+1]),0) + 1\n",
    "    extracted_net_flow_edge = {}\n",
    "    edge_list = []\n",
    "    extracted_select_node_set = set()\n",
    "    for k in edge_d:\n",
    "        edge_list.append((k[0],k[1],edge_d[k],100))\n",
    "        if k[0] not in extracted_net_flow_edge:\n",
    "            extracted_net_flow_edge[k[0]] = {}\n",
    "        extracted_net_flow_edge[k[0]][k[1]] = net_flow_edge[k[0]][k[1]]\n",
    "        extracted_select_node_set.add(k[0])\n",
    "        extracted_select_node_set.add(k[1])\n",
    "    extracted_select_node_set.add('@super target')    \n",
    "    extracted_net_flow_edge['@super target'] = {}\n",
    "    extracted_net_flow_edge[target_node]['@super target'] = 1000000\n",
    "    extracted_net_flow_edge['@super target'][target_node] = 1000000\n",
    "        \n",
    "    edge_list, node_list = run_network_flow('tmp',source_node,extracted_select_node_set,extracted_net_flow_edge,node2i,i2node)\n",
    "    return edge_list,node_list\n",
    "\n",
    "#print source_node,g\n",
    "#edge_list, node_list = dfs_find_graph(edge_type,source_node,g,select_node_set,net_flow_edge,node2i,i2node,node_type,6)\n",
    "\n",
    "def run_network_flow(fout_file,source_node,select_node_set,net_flow_edge,node2i,i2node):\n",
    "    edge_list = []\n",
    "    node_list = set()\n",
    "    nedge = -1\n",
    "    for edge_mult in [100]:\n",
    "        max_flow = pywrapgraph.SimpleMaxFlow()    \n",
    "        for g1 in select_node_set:\n",
    "            for g2 in net_flow_edge[g1]:\n",
    "                #if g2 not in select_node_set:\n",
    "                #    continue\n",
    "                #print node2i[g1],node2i[g2],net_flow_edge[g1][g2]\n",
    "                max_flow.AddArcWithCapacity(node2i[g1],node2i[g2],int(net_flow_edge[g1][g2]*edge_mult))\n",
    "                #print node2i[g1],node2i[g2],int(net_flow_edge[g1][g2]*edge_mult)\n",
    "                \n",
    "        if max_flow.Solve(node2i[source_node],node2i['@super target']) == max_flow.OPTIMAL:\n",
    "            fout = open(out_fname,'w')\n",
    "            fout.write('source\\ttarget\\tweight\\n')\n",
    "            #print('Max flow:', max_flow.OptimalFlow())\n",
    "            #print('')\n",
    "            #print('  Arc    Flow / Capacity')\n",
    "            nedge = 0\n",
    "            for i in range(max_flow.NumArcs()):\n",
    "                if max_flow.Flow(i)>0:\n",
    "                    nedge += 1\n",
    "                    fout.write(i2node[max_flow.Tail(i)]+'\\t'+i2node[max_flow.Head(i)]+'\\t'+str(max_flow.Flow(i))+'\\n')\n",
    "            fout.close()\n",
    "        else:\n",
    "            print('There was an issue with the max flow input.')\n",
    "        if nedge>1:\n",
    "            #print('Max flow:', max_flow.OptimalFlow())\n",
    "            #print('')\n",
    "            #print('  Arc    Flow / Capacity')\n",
    "            \n",
    "            \n",
    "            for i in range(max_flow.NumArcs()):\n",
    "                if max_flow.Flow(i)>0:\n",
    "                    edge_list.append([i2node[max_flow.Tail(i)],i2node[max_flow.Head(i)],max_flow.Flow(i),max_flow.Capacity(i)])\n",
    "                    node_list.add(i2node[max_flow.Tail(i)])\n",
    "                    node_list.add(i2node[max_flow.Head(i)])\n",
    "                    #print('%1s -> %1s   %3s  / %3s' % (i2node[max_flow.Tail(i)],i2node[max_flow.Head(i)],max_flow.Flow(i),max_flow.Capacity(i)))\n",
    "                    pass\n",
    "            break\n",
    "    return edge_list,node_list\n",
    "\n",
    "def extract_sub_graph(select_edge_ct,rev_select_edge_ct,source_node,target_set,max_path_L = 3, edge_thres = 0.1 ,max_ngh = 50):    \n",
    "    global dis2target,dis2source\n",
    "    \n",
    "    bp_netflow = collections.defaultdict(dict)\n",
    "    netflow = collections.defaultdict(dict)\n",
    "    get_K_source_ngh(source_node,select_edge_ct,max_path_L,edge_thres)\n",
    "                    \n",
    "    seed = set()\n",
    "    seed = seed.union(target_set)    \n",
    "    for s in seed:\n",
    "        get_K_target_ngh(s,rev_select_edge_ct,max_path_L,edge_thres,select_edge_ct)\n",
    "            \n",
    "    select_node_set = set()\n",
    "    for s in dis2source[source_node]:\n",
    "        dis2s = dis2source[source_node].get(s,100)\n",
    "        dis2t = 100\n",
    "        for t in target_set:\n",
    "            dis2t = min(dis2t,dis2target[t].get(s,100))\n",
    "        total_dis = dis2s + dis2t\n",
    "        if total_dis > max_path_L:\n",
    "            continue\n",
    "        select_node_set.add(s)\n",
    "        \n",
    "    select_node_set = select_node_set.union(seed)\n",
    "    select_node_set.add('@super target')\n",
    "    for t1 in select_node_set:\n",
    "        new_d = {}\n",
    "        for t2 in rev_select_edge_ct[t1]:            \n",
    "            if t2 in select_node_set:\n",
    "                #if t1==source_node:\n",
    "                #    print t1,t2,select_edge_ct[t1][t2],edge_thres\n",
    "                if rev_select_edge_ct[t1][t2] > edge_thres:\n",
    "                    new_d[t2] = rev_select_edge_ct[t1][t2] \n",
    "        sort_x = sorted(new_d.items(),key=operator.itemgetter(1))   \n",
    "        sort_x.reverse()\n",
    "        for i in range(min(len(sort_x),max_ngh)):\n",
    "            t2 = sort_x[i][0]\n",
    "            sc = sort_x[i][1]\n",
    "            bp_netflow[t1][t2] = sc\n",
    "            netflow[t2][t1] = sc\n",
    "        \n",
    "    for t in target_set:\n",
    "        netflow[t]['@super target'] = 1000000\n",
    "        netflow['@super target'][t] = 1000000\n",
    "\n",
    "    node2i = {}\n",
    "    i2node = {}\n",
    "    for s in select_node_set:\n",
    "        nnode = len(node2i)\n",
    "        i2node[nnode] = s\n",
    "        node2i[s] = nnode\n",
    "    \n",
    "    return netflow,node2i,i2node,select_node_set\n",
    "\n",
    "def get_auc_per_drug(gene_vec,d2g,d):\n",
    "    gene_score = gene_vec[d]\n",
    "    y = []\n",
    "    pred = []\n",
    "    #d = 'bladder'\n",
    "    for g in gene_score:\n",
    "        if g in d2g[d]:\n",
    "            y.append(1)\n",
    "        else:\n",
    "            y.append(0)\n",
    "        pred.append(gene_score[g])\n",
    "    y = np.array(y)\n",
    "    pred = np.array(pred)\n",
    "    if len(np.unique(y))<=1:\n",
    "        return float('nan')\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y, pred, pos_label=1)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    return auc\n",
    "\n",
    "def get_auc_per_gene(gene_vec,d2g,g):\n",
    "    y = []\n",
    "    pred = []\n",
    "    for d in gene_vec:\n",
    "        if g in gene_vec[d] and d in d2g:\n",
    "            if g in d2g[d]:\n",
    "                y.append(1)\n",
    "            else:\n",
    "                y.append(0)\n",
    "        pred.append(gene_vec[d][g])\n",
    "    y = np.array(y)\n",
    "    pred = np.array(pred)\n",
    "    if len(np.unique(y))<=1:\n",
    "        return float('nan')\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y, pred, pos_label=1)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    return auc\n",
    "\n",
    "def get_all_sentences(w1,w2):\n",
    "    nchunk = 400\n",
    "    sent_collection = []\n",
    "    os.chdir('/srv/local/work/swang141/PatientSetAnnotation/Sheng/src/network_flow/')\n",
    "    for pid in range(nchunk):\n",
    "        fin = open('../../data/pubmed/pmid2meta_autophrase.chunk'+str(pid))\n",
    "        ct = 0\n",
    "        for line in fin:\n",
    "            text = line.lower().translate(None,',?!:()=%>/[]').strip().strip('.').split('. ')\n",
    "            sent_l = text\n",
    "            for sent in sent_l:\n",
    "                if w1 in sent and w2 in sent:\n",
    "                    sent_collection.append(sent.replace('.','')+' .')\n",
    "        fin.close()\n",
    "        if pid%50==0:\n",
    "            #print 'read',pid,len(sent_collection)\n",
    "            pass\n",
    "    return sent_collection\n",
    "\n",
    "def get_path(edge,st,ed):\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    stemmer = PorterStemmer()    \n",
    "\n",
    "    nedge = len(edge)\n",
    "    net = {}\n",
    "    for e in edge:\n",
    "        tp = e.split('(')[0]\n",
    "        e1 = e.split('(')[1].split(',')[0].lstrip(' ')\n",
    "        e2 = e.split('(')[1].split(',')[1].split(')')[0].lstrip(' ')\n",
    "        if st == e2.split('-')[0]:\n",
    "            st = e2\n",
    "        if ed == e2.split('-')[0]:\n",
    "            ed = e2\n",
    "        #e1 = e1.split('-')[0].lstrip(' ')\n",
    "        #e2 = e2.split('-')[0].lstrip(' ')\n",
    "        if e2 not in net:\n",
    "            net[e2] = {}\n",
    "        net[e2][e1] = tp\n",
    "    if st not in net or ed not in net:\n",
    "        return 'wrong',[]\n",
    "    cur = ed\n",
    "    ed_path = [ed.split('-')[0]]\n",
    "    while cur!='ROOT-0':\n",
    "        next = net[cur].keys()[0]\n",
    "        ed_path.append(next.split('-')[0])\n",
    "        cur = next  \n",
    "    cur = st\n",
    "    st_path = [st.split('-')[0]]\n",
    "    while cur!='ROOT-0':\n",
    "        next = net[cur].keys()[0]       \n",
    "        st_path.append(next.split('-')[0])\n",
    "        cur = next    \n",
    "    find = False\n",
    "    for si in range(0,len(st_path)):\n",
    "        for ei in range(0,len(ed_path)):\n",
    "            if st_path[si]==ed_path[ei]:\n",
    "                find = True\n",
    "                break\n",
    "        if find:\n",
    "            break\n",
    "    path = ''\n",
    "    word_list = []\n",
    "    for i in range(0,si):\n",
    "        path += st_path[i] + ' '\n",
    "        if st_path[i] != st.split('-')[0]:\n",
    "            word_list.append(lmtzr.lemmatize(stemmer.stem(st_path[i].decode(\"utf8\"))))\n",
    "    for i in range(ei,-1,-1):\n",
    "        path += ed_path[i] + ' '\n",
    "        if ed_path[i]!=ed.split('-')[0]:\n",
    "            word_list.append(lmtzr.lemmatize(stemmer.stem(ed_path[i].decode(\"utf8\"))))\n",
    "    return path, word_list\n",
    "\n",
    "def find_sent(w1,w2,topk_word = 2):\n",
    "#w1 = 'hyperphosphatemia'\n",
    "#w2 = 'calcium acetate'\n",
    "    sent_l = get_all_sentences(w1,w2)\n",
    "    os.chdir('/srv/local/work/swang141/PatientSetAnnotation/Sheng/analysis/stanford_parser_java/stanford-parser-full-2017-06-09/')\n",
    "    new_w2 = w2.replace(' ','_')\n",
    "    new_w1 = w1.replace(' ','_')\n",
    "    input_path = 'input.txt'\n",
    "    #fin = open(input_path)\n",
    "    #sent_l = []\n",
    "    #for line in fin:\n",
    "    #    sent_l.append(line.strip())\n",
    "    #fin.close()\n",
    "    fout = open(input_path,'w')\n",
    "    for s in sent_l:\n",
    "        fout.write(s.rstrip().lstrip().replace(w2,new_w2).replace(w1,new_w1)+'\\n')\n",
    "    fout.close()\n",
    "    \n",
    "    output_path = 'output.txt'\n",
    "    command = 'java -Xmx2g -cp \"*\" edu.stanford.nlp.parser.nndep.DependencyParser -model edu/stanford/nlp/models/parser/nndep/english_UD.gz -textFile '+input_path+' -outFile '+output_path\n",
    "    output = subprocess.check_output(command, shell=True)\n",
    "\n",
    "    fin = open(output_path)\n",
    "    edge = []\n",
    "    path_l = []\n",
    "    vocab_dct = {}\n",
    "    stem_word_list_l = []\n",
    "    for line in fin:\n",
    "        text = line.strip()\n",
    "        if text == '':\n",
    "            path,stem_word_list = get_path(edge,new_w2,new_w1)\n",
    "            stem_word_list_l.append(stem_word_list)\n",
    "            path_l.append(path)\n",
    "            for w in stem_word_list:\n",
    "                vocab_dct[w] = vocab_dct.get(w,0) + 1\n",
    "            #print path\n",
    "            #print '----------------'\n",
    "            edge = []\n",
    "        else:\n",
    "            edge.append(text)\n",
    "    fin.close()\n",
    "    os.chdir('/srv/local/work/swang141/PatientSetAnnotation/Sheng/src/network_flow/')\n",
    "    vocab_list = sorted(vocab_dct.items(),key=operator.itemgetter(1))\n",
    "    vocab_list.reverse()\n",
    "    ct = 0\n",
    "    vocab_score = {}\n",
    "    for i in vocab_list:\n",
    "        ct = ct + 1\n",
    "        if ct > topk_word:\n",
    "            break\n",
    "        vocab_score[i[0]] = i[1]\n",
    "        print i[0],i[1]\n",
    "\n",
    "    nsent = len(sent_l)\n",
    "    sent_score = {}\n",
    "    best_sc = -1\n",
    "    best_sent = ''\n",
    "    best_path = ''\n",
    "    #print nsent,len(path_l),len(sent_l),len(edge)\n",
    "    assert(nsent == len(path_l))\n",
    "    for i in range(nsent):\n",
    "        npath_word = len(path_l[i].split(' '))\n",
    "        nsent_word = len(sent_l[i].split(' '))\n",
    "        word_norm = npath_word * 50 + nsent_word\n",
    "        sc = 0.\n",
    "        #for ww in path_l[i].split(' '):\n",
    "        #    if ww not in sent_l[i]:\n",
    "        #        print sent_l[i],path_l[i]\n",
    "        for w in stem_word_list_l[i]:\n",
    "            sc = sc + vocab_score.get(w,0)\n",
    "        sc = sc / word_norm\n",
    "        sent_score[sent_l[i]] = sc\n",
    "        if sc > best_sc:            \n",
    "            best_sc = sc\n",
    "            best_sent = sent_l[i]\n",
    "            best_path = path_l[i]\n",
    "    #sent_list = sorted(sent_score.items(),key=operator.itemgetter(1))\n",
    "    #sent_list.reverse()    \n",
    "    #for i in range(0,3):\n",
    "    #    print sent_list[i][0],sent_list[i][1]\n",
    "    return best_sent,best_path,best_sc,path_l\n",
    "\n",
    "\n",
    "def extract_sent(ect_ref,e1,e2):\n",
    "    if e1 in kg_relation and e2 in kg_relation[e1]:\n",
    "        sent = kg_relation[e1][e2][0]\n",
    "        return sent\n",
    "    pid,cid,sid,d = ect_ref[e1][e2].split('\\t')[0].split(',')\n",
    "    sid = int(sid)\n",
    "    cid = int(cid)\n",
    "    fin = open('../../data/pubmed/pmid2meta_autophrase.chunk'+pid)\n",
    "    ct = 0\n",
    "    for line in fin:\n",
    "        ct = ct + 1\n",
    "        if ct==int(cid):\n",
    "            sl = line.strip().split('.')\n",
    "            sent = sl[sid]\n",
    "            break\n",
    "    fin.close()\n",
    "    return sent\n",
    "\n",
    "def get_K_source_ngh(s,net,max_path_L,edge_thres): \n",
    "    global dis2source\n",
    "    if s in dis2source:\n",
    "        return \n",
    "    dis2source[s] = {}\n",
    "    visited = set()\n",
    "    visited.add(s)\n",
    "    dis2source[s][s] = 0\n",
    "    for iter in range(max_path_L):\n",
    "        new_node = set()\n",
    "        for ss in visited:\n",
    "            if ss not in net:\n",
    "                continue\n",
    "            for e in net[ss]:\n",
    "                if e in dis2source[s] or net[ss][e] < edge_thres:\n",
    "                    continue\n",
    "                new_node.add(e) \n",
    "        visited = new_node\n",
    "        for ss in visited:\n",
    "            if ss not in dis2source[s]:\n",
    "                dis2source[s][ss] = iter    \n",
    "    return\n",
    "                    \n",
    "def get_K_target_ngh(s,rev_net,max_path_L,edge_thres,net): \n",
    "    global dis2target\n",
    "    if s in dis2target:\n",
    "        return \n",
    "    dis2target[s] = {}\n",
    "    visited = set()\n",
    "    visited.add(s)\n",
    "    dis2target[s][s] = 0\n",
    "    for iter in range(max_path_L):\n",
    "        new_node = set()\n",
    "        for ss in visited:\n",
    "            if ss not in rev_net:\n",
    "                continue\n",
    "            for e in rev_net[ss]:\n",
    "                if e in dis2target[s]  or net[e][ss] < edge_thres:\n",
    "                    continue\n",
    "                new_node.add(e) \n",
    "        visited = new_node\n",
    "        for ss in visited:\n",
    "            if ss not in dis2target[s]:\n",
    "                dis2target[s][ss] = iter+1\n",
    "        \n",
    "    return\n",
    "\n",
    "def run_network_flow(fout_file,source_node,select_node_set,net_flow_edge,node2i,i2node):\n",
    "    edge_list = []\n",
    "    node_list = set()\n",
    "    nedge = -1\n",
    "    for edge_mult in [10]:\n",
    "        max_flow = pywrapgraph.SimpleMaxFlow()    \n",
    "        for g1 in select_node_set:\n",
    "            for g2 in net_flow_edge[g1]:\n",
    "                #if g2 not in select_node_set:\n",
    "                #    continue\n",
    "                #print node2i[g1],node2i[g2],net_flow_edge[g1][g2]\n",
    "                max_flow.AddArcWithCapacity(node2i[g1],node2i[g2],int(net_flow_edge[g1][g2]*edge_mult))\n",
    "                #print node2i[g1],node2i[g2],int(net_flow_edge[g1][g2]*edge_mult)\n",
    "                \n",
    "        if max_flow.Solve(node2i[source_node],node2i['@super target']) == max_flow.OPTIMAL:\n",
    "            fout = open(out_fname,'w')\n",
    "            fout.write('source\\ttarget\\tweight\\n')\n",
    "            #print('Max flow:', max_flow.OptimalFlow())\n",
    "            #print('')\n",
    "            #print('  Arc    Flow / Capacity')\n",
    "            nedge = 0\n",
    "            for i in range(max_flow.NumArcs()):\n",
    "                if max_flow.Flow(i)>0:\n",
    "                    nedge += 1\n",
    "                    fout.write(i2node[max_flow.Tail(i)]+'\\t'+i2node[max_flow.Head(i)]+'\\t'+str(max_flow.Flow(i))+'\\n')\n",
    "            fout.close()\n",
    "        else:\n",
    "            print('There was an issue with the max flow input.')\n",
    "        if nedge>1:\n",
    "            #print('Max flow:', max_flow.OptimalFlow())\n",
    "            #print('')\n",
    "            #print('  Arc    Flow / Capacity')\n",
    "            \n",
    "            \n",
    "            for i in range(max_flow.NumArcs()):\n",
    "                if max_flow.Flow(i)>0:\n",
    "                    edge_list.append([i2node[max_flow.Tail(i)],i2node[max_flow.Head(i)],max_flow.Flow(i),max_flow.Capacity(i)])\n",
    "                    node_list.add(i2node[max_flow.Tail(i)])\n",
    "                    node_list.add(i2node[max_flow.Head(i)])\n",
    "                    #print('%1s -> %1s   %3s  / %3s' % (i2node[max_flow.Tail(i)],i2node[max_flow.Head(i)],max_flow.Flow(i),max_flow.Capacity(i)))\n",
    "                    pass\n",
    "            break\n",
    "    return edge_list,node_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T18:09:52.605388Z",
     "start_time": "2018-03-01T18:02:12.061616Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000000 0.281889920069 8 no-reflow 1955 coronary interventions 1266\n",
      "60000000 0.563779840139 4 tubular networks 215 melanoma tumor cells 216\n",
      "90000000 0.845669760208 4 podoplanin 2190 inflammatory cell infiltration 4777\n"
     ]
    }
   ],
   "source": [
    "os.chdir('/srv/local/work/swang141/PatientSetAnnotation/Sheng/src/network_flow/')\n",
    "file = '../../result/processed_word_net/TCGA/aggregate'\n",
    "wct = {}\n",
    "ect = {}\n",
    "ect_ref = {}\n",
    "ct = 0\n",
    "fin = open(file)\n",
    "for line in fin:\n",
    "    w = line.strip().split('\\t')\n",
    "    w1,c1,w2,c2,c12 = w[0:5]\n",
    "    c12 = int(c12)\n",
    "    c1 = int(c1)\n",
    "    c2 = int(c2)\n",
    "    ct = ct + 1\n",
    "    if c12<=3:\n",
    "        continue\n",
    "    if w1 not in ect:\n",
    "        ect[w1] = {}\n",
    "        ect_ref[w1] = {}\n",
    "    if w2 not in ect:\n",
    "        ect[w2] = {}\n",
    "        ect_ref[w2] = {}\n",
    "    if ct%30000000==0:\n",
    "        print ct,ct*1.0/106424522,c12,w1,c1,w2,c2\n",
    "    ect[w1][w2] = c12\n",
    "    ect[w2][w1] = c12\n",
    "    wct[w1] = c1#wct.get(w1,0) + c12\n",
    "    wct[w2] = c2#wct.get(w2,0) + c12\n",
    "    #ect_ref[w1][w2] = '\\t'.join(w[5:])\n",
    "    #ect_ref[w2][w1] = '\\t'.join(w[5:])\n",
    "fin.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T03:33:12.805903Z",
     "start_time": "2018-03-02T03:33:12.627617Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "generate_stop_word_list = False\n",
    "stop_words = set()\n",
    "\n",
    "if generate_stop_word_list:        \n",
    "    gene_list = []\n",
    "    for g in node_type:\n",
    "        if node_type[g] == 'hpo_disease' and g in wct and wct[g]>10:\n",
    "            gene_list.append(g)\n",
    "    print len(gene_list)\n",
    "    fout = open('../../data/dictionary/stopwords.txt','w')\n",
    "    sort_x = sorted(wct.items(),key=operator.itemgetter(1))    \n",
    "    sort_x.reverse()    \n",
    "    new_d = {}\n",
    "    for i in range(50000):\n",
    "        w = sort_x[i][0]\n",
    "        p_list = []\n",
    "        q_list = []\n",
    "        for g in gene_list:\n",
    "            if g in ect[w]:\n",
    "                p_list.append(ect[w][g]*1.0)\n",
    "            else:\n",
    "                p_list.append(0.)\n",
    "            q_list.append(wct[g]*1.0)\n",
    "        p_list = np.array(p_list) / np.sum(np.array(p_list))\n",
    "        q_list = np.array(q_list) / np.sum(np.array(q_list))\n",
    "        kl = stats.entropy(p_list,q_list)\n",
    "        new_d[w] = kl\n",
    "    sort_x = sorted(new_d.items(),key=operator.itemgetter(1))   \n",
    "    for i in range(50000):\n",
    "        w = sort_x[i][0]\n",
    "        #print kl,np.sum(q_list),np.sum(p_list)\n",
    "        if w not in node_type:\n",
    "            node_type[w] = 'unknown'\n",
    "        fout.write(w+'\\t'+node_type[w]+'\\t'+str(sort_x[i][1])+'\\t'+str(wct[w])+'\\n')\n",
    "        stop_words.add(w)\n",
    "    fout.close()\n",
    "else:\n",
    "    fin = open('../../data/dictionary/stopwords.txt')\n",
    "    for line in fin:\n",
    "        w = line.strip().split('\\t')[0]\n",
    "        sc = float(line.strip().split('\\t')[2])\n",
    "        if sc > 0.9:\n",
    "            continue\n",
    "        stop_words.add(w)\n",
    "    fin.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T03:33:15.243280Z",
     "start_time": "2018-03-02T03:33:15.240429Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9205\n"
     ]
    }
   ],
   "source": [
    "print len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T18:23:57.627271Z",
     "start_time": "2018-03-01T18:23:37.788691Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "node_type = {}\n",
    "literome_g2g = collections.defaultdict(dict)\n",
    "###process gene layer\n",
    "fin = open('../../data/gene/pubmed.network')\n",
    "for line in fin:\n",
    "    g1,g2,type,conf = line.lower().strip().split('\\t')\n",
    "    literome_g2g[g1][g2] = 100\n",
    "    literome_g2g[g2][g1] = 100\n",
    "    node_type[g1] = 'gene'\n",
    "    node_type[g2] = 'gene'\n",
    "fin.close()\n",
    "\n",
    "fin = open('../../data/gene/description.txt')\n",
    "synonym_g2g = collections.defaultdict(dict)\n",
    "description_t2g = collections.defaultdict(dict)\n",
    "ct = 0\n",
    "for line in fin:\n",
    "    w = line.lower().strip().split('@')\n",
    "    if len(w)>3:\n",
    "        continue\n",
    "    id,gl,text = w\n",
    "    gl = gl.split(',')\n",
    "    for g1 in gl:\n",
    "        for g2 in gl:\n",
    "            if g1==g2 or g1 not in wct or g2 not in wct:\n",
    "                continue\n",
    "            synonym_g2g[g1][g2] = 100\n",
    "            synonym_g2g[g2][g1] = 100\n",
    "            node_type[g1] = 'gene'\n",
    "            node_type[g2] = 'gene'            \n",
    "    text = line.lower().translate(None, ',:()=%>/[]')\n",
    "    sent_l = text.strip().split('.')\n",
    "    sent_ct = 0\n",
    "    for sent in sent_l:\n",
    "        pset = {}\n",
    "        wl = sent.split(' ')\n",
    "        w = ''\n",
    "        for i in range(len(wl)):\n",
    "            tmp_w = ''\n",
    "            for k in range(5):\n",
    "                if i+k >= len(wl):\n",
    "                    break\n",
    "                tmp_w += wl[i+k]\n",
    "                if tmp_w in wct and tmp_w not in stop_words:\n",
    "                    pset[tmp_w] = i                            \n",
    "                tmp_w += ' '\n",
    "        for ti in pset.keys():\n",
    "            for tj in pset.keys():\n",
    "                if ti in tj and ti!=tj:\n",
    "                    pset.pop(ti, None)\n",
    "        for w in pset:\n",
    "            for g in gl:\n",
    "                description_t2g[w][g] = 100 \n",
    "                node_type[g] = 'gene'\n",
    "                node_type[w] = 'ncbi_function'\n",
    "                #print g,w\n",
    "    ct += 1\n",
    "    if ct%10000==0:\n",
    "        print ct\n",
    "fin.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T19:40:57.095531Z",
     "start_time": "2018-03-01T19:40:44.573457Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disease\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nif not use_disease:\\n    topk_tgt = 10\\n    result_dir = '../../result/network_flow/CTPR_drug/'\\n    d2dname = {}\\n    fin = open('../../data/drug/drug_map.txt')\\n    cancer2phrase = {}\\n    for line in fin:\\n        w = line.lower().strip().split('\\t')    \\n        d2dname[w[2]] = w[0]\\n    fin.close()\\n    d2g = {}\\n    gene_set_file = '../../data/drug/top_genes_exp_hgnc.txt'\\n    fin = open(gene_set_file)\\n    for line in fin:\\n        w = line.lower().strip().split('\\t')\\n        d = d2dname[w[1]]\\n        node_type[d] = 'drug'\\n        if w[0] not in ect:\\n            continue\\n        if d not in d2g:\\n            d2g[d] = {}\\n        d2g[d][w[0]] = abs(float(w[2]))\\n        node_type[w[0]] = 'gene'        \\nelif use_disease:\\n    topk_tgt = 1000\\n    result_dir = '../../result/network_flow/disease/'\\n    d2g = {}\\n    gene_set_file = '../../data/disease/gene_disease_formatted.txt'\\n    fin = open(gene_set_file)\\n    for line in fin:\\n        d,g = line.lower().strip().split('\\t')\\n        node_type[d] = 'disease'\\n        if d not in d2g:\\n            d2g[d] = {}\\n        if g not in ect or d not in ect:\\n            continue\\n        d2g[d][g] = 1.\\n        node_type[g] = 'gene'\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin = open('../../data/dictionary/AutoPhrase_multi-words.txt.only_GO_based')\n",
    "for line in fin:\n",
    "    w = line.lower().strip().split('\\t')\n",
    "    sc = float(w[0])\n",
    "    if sc<0.8:\n",
    "        continue\n",
    "    node_type[w[1]] = 'Han' + '_function'\n",
    "fin.close()\n",
    "\n",
    "fin = open('../../data/Relation/all_entity.txt')\n",
    "for line in fin:\n",
    "    w  =line.strip().lower().split('\\t')\n",
    "    if w[1] == 'disease':\n",
    "        w[1] = 'disease'\n",
    "    if w[0].replace('_',' ') in node_type:\n",
    "        node_type[w[0].replace('_',' ')] ='Percha_'+w[1]\n",
    "    else:\n",
    "        node_type[w[0].replace('_',' ')] ='Percha_'+w[1]\n",
    "fin.close()\n",
    "\n",
    "\n",
    "fin = open('../../data/dictionary/hpo/20180118/phenotype_annotation.tab')\n",
    "for line in fin:\n",
    "    w = line.strip().lower().split('\\t')\n",
    "    node_type[w[2]] = 'hpo_disease'\n",
    "fin.close()\n",
    "\n",
    "fin = open('../../data/dictionary/hpo/20180118/hp_obo_parsed.tsv')\n",
    "for line in fin:\n",
    "    w = line.strip().lower().split('\\t')\n",
    "    w1 = w[0].split('(')[1].replace(')','')\n",
    "    w2 = w[1].split('(')[1].replace(')','')\n",
    "    node_type[w1] = 'hpo_disease'\n",
    "    node_type[w2] = 'hpo_disease'\n",
    "fin.close()\n",
    "\n",
    "fin = open('../../data/disease/tissue.txt')\n",
    "for line in fin:\n",
    "    w =line.strip().lower()\n",
    "    node_type[w] = 'tissue'\n",
    "fin.close()\n",
    "\n",
    "mesh_mapping = {'A':'tissue','B':'tissue','C':'disease','D':'drug','E':'drug','F':'disease',\n",
    "               'G':'function','H':'entity','I':'entity','J':'entity','K':'entity','M':'entity','L':'entity',\n",
    "               'N':'entity','V':'entity','Z':'entity'}\n",
    "fin = open('../../data/dictionary/2017MeshTree.txt')\n",
    "fin.readline()\n",
    "for line in fin:\n",
    "    w =line.strip().lower().split('\\t')\n",
    "    t = mesh_mapping[w[0][0].upper()]\n",
    "    node_type[w[2]] = 'mesh' + '_' + t\n",
    "fin.close()\n",
    "\n",
    "fin = open('../../data/dictionary/go_term.txt')\n",
    "for line in fin:\n",
    "    w  =line.strip().lower()\n",
    "    node_type[w] = 'GO_function'\n",
    "fin.close()\n",
    "\n",
    "\n",
    "gene_set_file = '../../data/drug/top_genes_exp_hgnc.txt'\n",
    "fin = open(gene_set_file)\n",
    "for line in fin:\n",
    "    w = line.lower().strip().split('\\t')\n",
    "    node_type[w[0]] = 'gene' \n",
    "\n",
    "\n",
    "source_set = set()\n",
    "use_disease = True\n",
    "\n",
    "fin = open('../../data/disease/cancer_gene.txt')\n",
    "result_dir = '../../result/network_flow/cancer/'\n",
    "d2g = {}\n",
    "background_gene = set()\n",
    "for line in fin:\n",
    "    w = line.lower().strip().split('\\t')    \n",
    "    if w[1] not in ect:\n",
    "        continue\n",
    "    if w[1] not in d2g:\n",
    "        d2g[w[1]] = set()\n",
    "    d2g[w[1]].add(w[0])\n",
    "    node_type[w[0]] = 'gene'\n",
    "    node_type[w[1]] = 'disease'\n",
    "    background_gene.add(w[0])\n",
    "fin.close()\n",
    "\n",
    "\n",
    "print node_type['lung adenocarcinoma']\n",
    "\n",
    "\n",
    "'''\n",
    "if not use_disease:\n",
    "    topk_tgt = 10\n",
    "    result_dir = '../../result/network_flow/CTPR_drug/'\n",
    "    d2dname = {}\n",
    "    fin = open('../../data/drug/drug_map.txt')\n",
    "    cancer2phrase = {}\n",
    "    for line in fin:\n",
    "        w = line.lower().strip().split('\\t')    \n",
    "        d2dname[w[2]] = w[0]\n",
    "    fin.close()\n",
    "    d2g = {}\n",
    "    gene_set_file = '../../data/drug/top_genes_exp_hgnc.txt'\n",
    "    fin = open(gene_set_file)\n",
    "    for line in fin:\n",
    "        w = line.lower().strip().split('\\t')\n",
    "        d = d2dname[w[1]]\n",
    "        node_type[d] = 'drug'\n",
    "        if w[0] not in ect:\n",
    "            continue\n",
    "        if d not in d2g:\n",
    "            d2g[d] = {}\n",
    "        d2g[d][w[0]] = abs(float(w[2]))\n",
    "        node_type[w[0]] = 'gene'        \n",
    "elif use_disease:\n",
    "    topk_tgt = 1000\n",
    "    result_dir = '../../result/network_flow/disease/'\n",
    "    d2g = {}\n",
    "    gene_set_file = '../../data/disease/gene_disease_formatted.txt'\n",
    "    fin = open(gene_set_file)\n",
    "    for line in fin:\n",
    "        d,g = line.lower().strip().split('\\t')\n",
    "        node_type[d] = 'disease'\n",
    "        if d not in d2g:\n",
    "            d2g[d] = {}\n",
    "        if g not in ect or d not in ect:\n",
    "            continue\n",
    "        d2g[d][g] = 1.\n",
    "        node_type[g] = 'gene'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1089,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disease\n"
     ]
    }
   ],
   "source": [
    "edge_type = []\n",
    "edge_type.append(['disease','symptom'])\n",
    "edge_type.append(['disease','tissue'])\n",
    "edge_type.append(['disease','function'])\n",
    "edge_type.append(['disease','disease'])\n",
    "edge_type.append(['disease','gene'])\n",
    "\n",
    "edge_type.append(['tissue','function'])\n",
    "edge_type.append(['tissue','gene'])\n",
    "edge_type.append(['tissue','symptom'])\n",
    "edge_type.append(['tissue','tissue'])\n",
    "\n",
    "edge_type.append(['symptom','function'])\n",
    "edge_type.append(['symptom','tissue'])\n",
    "edge_type.append(['symptom','symptom'])\n",
    "edge_type.append(['symptom','gene'])\n",
    "\n",
    "#edge_type.append(['function','gene'])\n",
    "edge_type.append(['function','function'])\n",
    "\n",
    "#edge_type.append(['gene','gene'])\n",
    "print node_type['lung adenocarcinoma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T03:34:27.207276Z",
     "start_time": "2018-03-02T03:33:47.628592Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disease\n"
     ]
    }
   ],
   "source": [
    "kg_thres = 50000\n",
    "kg_relation = collections.defaultdict(dict)\n",
    "fin = open('../../data/Relation/all_relation.txt')\n",
    "\n",
    "for line in fin:\n",
    "    w  =line.strip().lower().split('\\t')\n",
    "    w1 = w[0]\n",
    "    w2 = w[1]\n",
    "    if w1 in stop_words or w2 in stop_words:\n",
    "        continue\n",
    "    if w1 not in node_type or w2 not in node_type:\n",
    "        continue\n",
    "    if len(w1) < 5 and (w1 not in node_type or node_type[w1]=='entity'):\n",
    "        continue\n",
    "    if len(w2) < 5 and (w2 not in node_type or node_type[w2]=='entity'):\n",
    "        continue\n",
    "    conf = float(w[3])\n",
    "    conf = 100\n",
    "    if conf < kg_thres:\n",
    "        continue\n",
    "    sent = w[4]\n",
    "    if [node_type[w1],node_type[w2]] in edge_type:\n",
    "        kg_relation[w1][w2] = (sent,conf)\n",
    "        continue\n",
    "    if [node_type[w2],node_type[w1]] in edge_type:\n",
    "        kg_relation[w2][w1] = (sent,conf)\n",
    "        continue\n",
    "fin.close()\n",
    "\n",
    "\n",
    "gene_ontology = collections.defaultdict(dict)\n",
    "\n",
    "fin = open('../../data/Relation/GO_term.network')\n",
    "for line in fin:\n",
    "    w  =line.strip().lower().split('\\t')\n",
    "    w1 = w[0]\n",
    "    w2 = w[1]\n",
    "    gene_ontology[w1][w2] = 100\n",
    "fin.close()\n",
    "\n",
    "print node_type['lung adenocarcinoma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T04:18:53.194333Z",
     "start_time": "2018-03-02T04:14:10.776677Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished 0.133534168723\n",
      "finished 0.267068337446\n",
      "finished 0.400602506169\n",
      "finished 0.534136674892\n",
      "finished 0.667670843615\n",
      "finished 0.801205012339\n",
      "finished 0.934739181062\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nfor g in kg_relation:\\n    for t in kg_relation[g]:        \\n        bp_net[t][g] = kg_relation[g][t][1]\\n        net[g][t] = kg_relation[g][t][1]\\n   \\ndatabase_relation_l = [gene_ontology,description_t2g,literome_g2g]\\n\\nfor dr in database_relation_l:\\n    for g in dr:\\n        for t in dr[g]:\\n            net[g][t] = dr[g][t]\\n            bp_net[t][g] = dr[g][t]\\n'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "net = collections.defaultdict(dict)\n",
    "bp_net = collections.defaultdict(dict)\n",
    "topk = 5000\n",
    "sent_ct = 239833652\n",
    "ct = 0\n",
    "max_ngh = 100\n",
    "for k1 in ect:\n",
    "    ct += 1\n",
    "    if k1 in stop_words:\n",
    "        continue\n",
    "    new_d = {}\n",
    "    for k2 in ect[k1]:\n",
    "        if k2 in stop_words:\n",
    "            continue\n",
    "        #if k2 not in node_type or k1 not in node_type:\n",
    "        #    continue\n",
    "        #if False:#[node_type[k1],node_type[k2]] not in edge_type:\n",
    "        #    continue\n",
    "        #if node_type[k1] == node_type[k2] and wct[k1] < wct[k2]:\n",
    "            #new_d[k2] = -1            \n",
    "        #else:\n",
    "        value = ect[k1][k2]*1.0 / wct[k2] / wct[k1] *239833652  \n",
    "        if value <= 20:\n",
    "            continue\n",
    "        #table = np.zeros((2,2))        \n",
    "        #table[0,1] = wct[k2] - ect[k1][k2]\n",
    "        #table[1,0] = wct[k1] - ect[k1][k2]\n",
    "        #table[1,1] = ect[k1][k2]\n",
    "        #table[0,0] = sent_ct - table[0,1] - table[1,1] - table[1,0]\n",
    "        #oddsratio, pvalue = stats.fisher_exact([[8, 2], [1, 5]])\n",
    "        #new_d[k2] = - np.log10(pvalue)\n",
    "        #if ect[k1][k2]*1.0 / wct[k2]<0.01:\n",
    "        #    continue\n",
    "        new_d[k2] = value\n",
    "    sort_x = sorted(new_d.items(),key=operator.itemgetter(1))   \n",
    "    sort_x.reverse()\n",
    "    for i in range(min(len(sort_x),max_ngh)):\n",
    "        k2 = sort_x[i][0]\n",
    "        sc = sort_x[i][1]            \n",
    "        net[k1][k2] = sc\n",
    "        bp_net[k2][k1] =sc\n",
    "    if ct%100000==0:\n",
    "        print 'finished',ct*1.0/len(ect)\n",
    "\n",
    "'''\n",
    "for g in kg_relation:\n",
    "    for t in kg_relation[g]:        \n",
    "        bp_net[t][g] = kg_relation[g][t][1]\n",
    "        net[g][t] = kg_relation[g][t][1]\n",
    "   \n",
    "database_relation_l = [gene_ontology,description_t2g,literome_g2g]\n",
    "\n",
    "for dr in database_relation_l:\n",
    "    for g in dr:\n",
    "        for t in dr[g]:\n",
    "            net[g][t] = dr[g][t]\n",
    "            bp_net[t][g] = dr[g][t]\n",
    "'''         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "node_type['c3orf21']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1309,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1309-7924d6cc90cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mect\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent_ct\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0moddsratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfisher_exact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mpvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mect\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mwct\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mwct\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;36m239833652\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/local/work/swang141/software/Anaconda/anaconda/lib/python2.7/site-packages/scipy/stats/stats.pyc\u001b[0m in \u001b[0;36mfisher_exact\u001b[0;34m(table, alternative)\u001b[0m\n\u001b[1;32m   3164\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0moddsratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpupper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3166\u001b[0;31m             \u001b[0mguess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbinary_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lower\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3167\u001b[0m             \u001b[0mpvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpupper\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhypergeom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mguess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3168\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/local/work/swang141/software/Anaconda/anaconda/lib/python2.7/site-packages/scipy/stats/stats.pyc\u001b[0m in \u001b[0;36mbinary_search\u001b[0;34m(n, n1, n2, side)\u001b[0m\n\u001b[1;32m   3132\u001b[0m                 \u001b[0mguess\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3134\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mhypergeom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpmf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mguess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mpexact\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3135\u001b[0m                 \u001b[0mguess\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3136\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mguess\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhypergeom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpmf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mguess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mpexact\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/local/work/swang141/software/Anaconda/anaconda/lib/python2.7/site-packages/scipy/stats/_distn_infrastructure.pyc\u001b[0m in \u001b[0;36mpmf\u001b[0;34m(self, k, *args, **kwds)\u001b[0m\n\u001b[1;32m   2838\u001b[0m         \u001b[0mcond\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcond0\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mcond1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2839\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'd'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2840\u001b[0;31m         \u001b[0mplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mcond0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbadvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2841\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2842\u001b[0m             \u001b[0mgoodargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margsreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#k1 = 'lung cancer'\n",
    "k1  = 'mutant egfr'\n",
    "k2 = 'lung adenocarcinoma'\n",
    "table = np.zeros((2,2))        \n",
    "table[0,1] = wct[k2] - ect[k1][k2]\n",
    "table[1,0] = wct[k1] - ect[k1][k2]\n",
    "table[1,1] = ect[k1][k2]\n",
    "table[0,0] = sent_ct - table[0,1] - table[1,1] - table[1,0]\n",
    "oddsratio, pvalue = stats.fisher_exact(table)\n",
    "print pvalue\n",
    "print ect[k1][k2]*1.0 / wct[k2] / wct[k1] *239833652  \n",
    "print table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lung adenocarcinoma\n",
      "nnode 1283\n",
      "4 10 zmym2 lung adenocarcinoma -1 True nan 0\n",
      "nnode 2571\n",
      "4 10 plcb1 lung adenocarcinoma 328508.0 False 0.0 13\n",
      "nnode 7006\n",
      "4 10 alk lung adenocarcinoma 61989.0 False 0.0 7\n",
      "nnode 1528\n",
      "4 10 syncrip lung adenocarcinoma -1 True 0.0 0\n",
      "nnode 1212\n",
      "4 10 cct5 lung adenocarcinoma 35986.0 True 0.0 5\n",
      "nnode 3776\n",
      "4 10 trip10 lung adenocarcinoma -1 False 0.22222222222222224 0\n",
      "nnode 9292\n",
      "4 10 cdkn2a lung adenocarcinoma 127057.0 True 0.3333333333333333 5\n",
      "nnode 5763\n",
      "4 10 syk lung adenocarcinoma -1 False 0.4375 0\n",
      "nnode 1916\n",
      "4 10 hcfc1 lung adenocarcinoma -1 False 0.49999999999999994 0\n",
      "nnode 9073\n",
      "4 10 rac1 lung adenocarcinoma 49767.0 False 0.45833333333333337 5\n",
      "nnode 2408\n",
      "4 10 rfc4 lung adenocarcinoma -1 False 0.5 0\n",
      "nnode 1633\n",
      "4 10 magi2 lung adenocarcinoma -1 False 0.53125 0\n",
      "nnode 3474\n",
      "4 10 hsp90aa1 lung adenocarcinoma -1 True 0.4875 0\n",
      "nnode 2633\n",
      "4 10 tfdp2 lung adenocarcinoma -1 False 0.5111111111111111 0\n",
      "nnode 2746\n",
      "4 10 tfdp1 lung adenocarcinoma 18355.0 True 0.537037037037037 7\n",
      "nnode 2149\n",
      "4 10 dis3 lung adenocarcinoma -1 False 0.5583333333333332 0\n",
      "nnode 10656\n",
      "4 10 fas lung adenocarcinoma -1 False 0.5757575757575757 0\n",
      "nnode 748\n",
      "4 10 pip5k1a lung adenocarcinoma -1 True 0.5454545454545455 0\n",
      "nnode 3153\n",
      "4 10 rbm10 lung adenocarcinoma -1 True 0.5227272727272727 0\n",
      "nnode 5068\n",
      "4 10 bap1 lung adenocarcinoma -1 True 0.5050505050505051 0\n",
      "nnode 3002\n",
      "4 10 arap3 lung adenocarcinoma 38158.0 False 0.47222222222222227 5\n",
      "nnode 2078\n",
      "4 10 clasp2 lung adenocarcinoma -1 True 0.4583333333333334 0\n",
      "nnode 2482\n",
      "4 10 psma6 lung adenocarcinoma -1 True 0.446969696969697 0\n",
      "nnode 2991\n",
      "4 10 prrx1 lung adenocarcinoma -1 True 0.43750000000000006 0\n",
      "nnode 2357\n",
      "4 10 gps2 lung adenocarcinoma 49181.0 False 0.4102564102564103 4\n",
      "nnode 1567\n",
      "4 10 eef1b2 lung adenocarcinoma -1 True 0.4023668639053255 0\n",
      "nnode 3373\n",
      "4 10 fkbp5 lung adenocarcinoma 7046.0 False 0.3901098901098901 5\n",
      "nnode 3129\n",
      "4 10 med12 lung adenocarcinoma 22960.0 False 0.37435897435897436 5\n",
      "nnode 2794\n",
      "4 10 map3k4 lung adenocarcinoma -1 True 0.36666666666666664 0\n",
      "nnode 1096\n",
      "4 10 med17 lung adenocarcinoma -1 True 0.36 0\n",
      "nnode 3280\n",
      "4 10 cul1 lung adenocarcinoma -1 False 0.375 0\n",
      "nnode 2538\n",
      "4 10 cul2 lung adenocarcinoma 105388.0 True 0.41015625 5\n",
      "nnode 2506\n",
      "4 10 cul3 lung adenocarcinoma -1 True 0.4025735294117647 0\n",
      "nnode 2436\n",
      "4 10 pom121 lung adenocarcinoma -1 False 0.41522491349480967 0\n",
      "nnode 4219\n",
      "4 10 clock lung adenocarcinoma 105388.0 False 0.39705882352941174 5\n",
      "nnode 2971\n",
      "4 10 usp6 lung adenocarcinoma 23906.0 False 0.3854489164086688 5\n",
      "nnode 4451\n",
      "4 10 arid2 lung adenocarcinoma 104872.0 False 0.3720588235294118 5\n",
      "nnode 2224\n",
      "4 10 ahnak lung adenocarcinoma 27323.0 False 0.36274509803921573 5\n",
      "nnode 2369\n",
      "4 10 mat2a lung adenocarcinoma -1 False 0.37433155080213903 0\n",
      "nnode 3735\n",
      "4 10 hspa8 lung adenocarcinoma 15749.0 True 0.3838383838383838 5\n",
      "nnode 2293\n",
      "4 10 bclaf1 lung adenocarcinoma -1 False 0.39492753623188404 0\n",
      "nnode 2300\n",
      "4 10 zfhx3 lung adenocarcinoma 107583.0 False 0.380787037037037 9\n",
      "nnode 6268\n",
      "4 10 met lung adenocarcinoma -1 True 0.3739035087719298 0\n",
      "nnode 2378\n",
      "4 10 sfpq lung adenocarcinoma 158083.0 True 0.403125 5\n",
      "nnode 3047\n",
      "4 10 phf6 lung adenocarcinoma 158083.0 True 0.42956349206349204 5\n",
      "nnode 3054\n",
      "4 10 ybx1 lung adenocarcinoma 175771.0 False 0.41238095238095235 11\n",
      "nnode 202\n",
      "4 10 cyth4 lung adenocarcinoma -1 False 0.42216117216117216 0\n",
      "nnode 1121\n",
      "4 10 cntnap1 lung adenocarcinoma 29508.0 False 0.41534391534391535 5\n",
      "nnode 4164\n",
      "4 10 ppm1d lung adenocarcinoma -1 False 0.42431972789115646 0\n",
      "nnode 1082\n",
      "4 10 hnrpdl lung adenocarcinoma -1 False 0.43267651888341535 0\n",
      "nnode 2662\n",
      "4 10 acvr1b lung adenocarcinoma -1 True 0.4247648902821316 0\n",
      "nnode 5008\n",
      "4 10 sf3b1 lung adenocarcinoma 26347.0 True 0.43328335832083953 5\n",
      "nnode 2946\n",
      "4 10 ppp6c lung adenocarcinoma 33532.0 False 0.4260869565217391 5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1346-d303e1edbf17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m                     \u001b[0mtarget_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                     \u001b[0msource_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                     \u001b[0mnet_flow_edge\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnode2i\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi2node\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mselect_node_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_sub_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbp_net\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msource_node\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0medge_thres\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_ngh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                     \u001b[0medge_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                     \u001b[0mnedge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1344-4cb442f1d312>\u001b[0m in \u001b[0;36mextract_sub_graph\u001b[0;34m(select_edge_ct, rev_select_edge_ct, source_node, target_set, max_path_L, edge_thres, max_ngh)\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0mget_K_target_ngh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrev_select_edge_ct\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_path_L\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0medge_thres\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mselect_edge_ct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0mselect_node_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1344-4cb442f1d312>\u001b[0m in \u001b[0;36mget_K_target_ngh\u001b[0;34m(s, rev_net, max_path_L, edge_thres, net)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdis2target\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;32mor\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mss\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0medge_thres\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m                 \u001b[0mnew_node\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m         \u001b[0mvisited\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mss\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvisited\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#rank genes\n",
    "max_ngh = 10\n",
    "for max_path in [4]:\n",
    "    for edge_thres in [100]:\n",
    "        dis2source = {}\n",
    "        dis2target = {}\n",
    "        if not os.path.exists(result_dir):\n",
    "            os.makedirs(result_dir)\n",
    "        dct = 0\n",
    "        gene_score = {}\n",
    "        for d in d2g:\n",
    "            dct = dct + 1\n",
    "            if d!='lung adenocarcinoma':\n",
    "                continue\n",
    "            print d\n",
    "            if d not in net:\n",
    "                continue     \n",
    "            gene_score[d] = {}\n",
    "            gct = 0.\n",
    "            for g in background_gene:\n",
    "                gct += 1\n",
    "                #if g!='zmym2':\n",
    "                #    continue\n",
    "                if g in bp_net and len(bp_net[g])>0:\n",
    "                    target_set = set()\n",
    "                    target_set.add(g)\n",
    "                    source_node = d\n",
    "                    net_flow_edge,node2i,i2node,select_node_set = extract_sub_graph(net,bp_net,source_node,target_set,max_path,edge_thres,max_ngh)  \n",
    "                    edge_weight = -1\n",
    "                    nedge = 0\n",
    "                    if source_node not in select_node_set:\n",
    "                        #print source_node,'not found',g,len(select_node_set)\n",
    "                        pass\n",
    "                    else:\n",
    "                        print 'nnode',len(select_node_set)\n",
    "                        out_fname = result_dir + d+'_'+g\n",
    "                        #edge_list, node_list,net_edge_weight = dfs_with_regard_source_target(source_node,g,max_path)\n",
    "                        edge_list, node_list = dfs_find_graph(edge_type,source_node,'@super target',\n",
    "                                                              select_node_set,net_flow_edge,node2i,i2node,node_type,max_path+1)\n",
    "                        #edge_list, node_list = run_network_flow(out_fname,source_node,select_node_set,net_flow_edge,node2i,i2node)\n",
    "                        nedge = len(edge_list) \n",
    "                        type_d = {}\n",
    "                        for e in edge_list:        \n",
    "                            e1,e2,w,mw = e\n",
    "                            if e1 in node_type and e2 in node_type:\n",
    "                                type_d[(node_type[e1],node_type[e2])] = type_d.get((node_type[e1],node_type[e2]),0) + 1\n",
    "                            #print e1,e2,w,mw\n",
    "                            if e1=='@super target' or e2=='@super target':\n",
    "                                edge_weight = float(w)\n",
    "                            #if e2==g:\n",
    "                            #    print e1,e2,w,mw,select_edge_ct[e1][g]\n",
    "                            #if e1==d:\n",
    "                            #    print e1,e2,w,mw,select_edge_ct[e1][e2]\n",
    "                        #sort_x = sorted(type_d.items(),key=operator.itemgetter(1))   \n",
    "                        #sort_x.reverse()\n",
    "                        #for i in range(len(sort_x)):\n",
    "                        #    print sort_x[i][0],sort_x[i][1]\n",
    "                        \n",
    "                        plot_figure(node_list,edge_list,result_dir + d+'_'+g,net_flow_edge)\n",
    "                        \n",
    "                    if True:#edge_weight < 10000:# and edge_weight!=-1:\n",
    "                        gene_score[d][g] = edge_weight\n",
    "                        print max_path,max_edge,g,d,edge_weight,g in d2g[d],get_auc_per_drug(gene_score,d2g,d),nedge\n",
    "            print max_path,max_edge,d,get_auc_per_drug(gene_score,d2g,d)\n",
    "\n",
    "            #IFrame(\"test.pdf\", width=600, height=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30941"
      ]
     },
     "execution_count": 1347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net['breast']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sema6d 75 290707.45697 0.0533333333333 0.0909090909091\n",
      "tcga 2210 64126.6449198 0.0117647058824 0.590909090909\n",
      "cancer genome atlas 2089 52185.3979721 0.00957395883198 0.454545454545\n",
      "brca 1851 47116.2815186 0.00864397622907 0.363636363636\n",
      "lung squamous cell carcinoma 849 25680.870757 0.00471142520612 0.0909090909091\n",
      "primary chemotherapy 1781 12242.0321576 0.00224592925323 0.0909090909091\n",
      "carcinoma cases 1912 11403.2736782 0.00209205020921 0.0909090909091\n",
      "regulatory subunits 2236 9750.92096276 0.00178890876565 0.0909090909091\n",
      "glioblastoma multiforme 9340 5835.93663617 0.00107066381156 0.227272727273\n",
      "lung adenocarcinoma 6502 3353.28503118 0.000615195324516 0.0909090909091\n",
      "c-erbb-2 7533 2894.339476 0.000530996946768 0.0909090909091\n",
      "breast cancer cell lines 10035 2172.70147212 0.00039860488291 0.0909090909091\n",
      "gbm 20230 1616.63810722 0.000296589223925 0.136363636364\n",
      "dataset 37829 1440.89582547 0.000264347458299 0.227272727273\n",
      "patient survival 24811 878.765840665 0.000161218814236 0.0909090909091\n",
      "highly expressed 30941 704.665630481 0.000129278303869 0.0909090909091\n",
      "bcl-2 91239 238.966442779 4.38409013689e-05 0.0909090909091\n",
      "p53 106045 205.601954573 3.77198359187e-05 0.0909090909091\n"
     ]
    }
   ],
   "source": [
    "ct = 0\n",
    "sent_ct = 239833652\n",
    "w = 'breast invasive carcinoma'\n",
    "new_d = {}\n",
    "for w1 in net[w]:\n",
    "    new_d[w1] = ect[w][w1] * 1.0/ wct[w1] / wct[w] * sent_ct\n",
    "sort_x = sorted(new_d.items(),key=operator.itemgetter(1))   \n",
    "sort_x.reverse()\n",
    "for i in range(min(len(sort_x),50)):\n",
    "    print sort_x[i][0],wct[sort_x[i][0]],sort_x[i][1],ect[w][sort_x[i][0]] * 1.0/ wct[sort_x[i][0]],ect[w][sort_x[i][0]] * 1.0/ wct[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 1353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bp_net[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3795"
      ]
     },
     "execution_count": 1270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(select_node_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 987,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words.add('snps')\n",
    "stop_words.add('snp')\n",
    "\n",
    "def combine_str(str1,str2):\n",
    "    s1 = str1.split('#')\n",
    "    s2 = str2.split('#')\n",
    "    s = ''\n",
    "    for i in range(0,len(s1)-1):\n",
    "        s += s1[i]+'#'\n",
    "    s += str2\n",
    "    return s\n",
    "\n",
    "def dfs_find_strong_evidence_graph(source_node,target_node,ect,depth,used_node):\n",
    "    if depth==0:\n",
    "        return [source_node + '#' + target_node]\n",
    "    all_path = set()\n",
    "    sc = {}\n",
    "    for w in wct:\n",
    "        if source_node not in node_type or w not in node_type or target_node not in node_type:\n",
    "            continue\n",
    "        if [node_type[source_node],node_type[w]] not in edge_type:\n",
    "            continue\n",
    "        if [node_type[w],node_type[target_node]] not in edge_type:\n",
    "            continue\n",
    "        if node_type[w] == node_type[target_node] and node_type[w]!='gene' and wct[w] < wct[target_node]:\n",
    "            continue\n",
    "        if node_type[source_node] == node_type[w] and node_type[w]!='gene' and wct[source_node] < wct[w]:\n",
    "            continue            \n",
    "        if w in ect[source_node] and w in ect[target_node] and w not in stop_words and w not in used_node:\n",
    "            sc[w] = ect[source_node][w]*ect[target_node][w]*1.0/wct[target_node]/wct[w]/wct[source_node]/wct[w]\n",
    "    sort_x = sorted(sc.items(),key=operator.itemgetter(1))   \n",
    "    sort_x.reverse()\n",
    "    if len(sort_x)==0:\n",
    "        return \"\"\n",
    "    w = sort_x[0][0]\n",
    "    next_used_node = set()\n",
    "    for s in used_node:\n",
    "        next_used_node.add(s)\n",
    "    next_used_node.add(w)\n",
    "    path_tgt = dfs_find_strong_evidence_graph(w,target_node,ect,depth-1,next_used_node)\n",
    "    path_src = dfs_find_strong_evidence_graph(source_node,w,ect,depth-1,next_used_node)\n",
    "    #print w,path,source_node,target_node\n",
    "    '''\n",
    "    if type(path_tgt)==str:\n",
    "        all_path.append(source_node + '#' + path)\n",
    "    else:\n",
    "        for p in path:\n",
    "            all_path.append(source_node + '#' + p)\n",
    "    '''\n",
    "    #print 'tgt',depth,path_tgt,w,target_node\n",
    "    #print 'src',depth,path_src,source_node,w\n",
    "    for pt in path_tgt:\n",
    "        for ps in path_src:\n",
    "            all_path.add(combine_str(ps,pt))\n",
    "            #print 'cb',depth,combine_str(ps,pt)\n",
    "    for pt in path_tgt:\n",
    "        all_path.add(source_node+'#'+pt)\n",
    "        #print 'cb',depth,source_node+'#'+pt\n",
    "    for ps in path_src:\n",
    "        all_path.add(ps+'#'+target_node)\n",
    "        #print 'cb',depth,ps+'#'+target_node \n",
    "    if target_node in ect[source_node]:\n",
    "        all_path.add(source_node+'#'+target_node)\n",
    "    return all_path\n",
    "\n",
    "def dfs_with_regard_source_target(source_node,target_node,path_depth=3):\n",
    "    #source_node = 'lung adenocarcinoma'\n",
    "    #target_node = 'hsp90aa1'\n",
    "    used_node = set()\n",
    "    all_path = dfs_find_strong_evidence_graph(source_node,target_node,ect,path_depth,used_node)\n",
    "    extracted_net_flow_edge = {}\n",
    "    extracted_select_node_set = set()\n",
    "    extracted_net_flow_edge[target_node] = {}\n",
    "    for path in all_path:\n",
    "        w = path.split('#')\n",
    "        #print path\n",
    "        for i in range(0,len(w)-1):\n",
    "            w1 = w[i]\n",
    "            w2 = w[i+1]\n",
    "            if w1 not in extracted_net_flow_edge:\n",
    "                extracted_net_flow_edge[w1] = {}\n",
    "            extracted_net_flow_edge[w1][w2] = ect[w1][w2]*1000000.0/(wct[w1]+1)/(wct[w2]+1)\n",
    "            extracted_select_node_set.add(w1)\n",
    "            extracted_select_node_set.add(w2)        \n",
    "\n",
    "    extracted_select_node_set.add('@super target')    \n",
    "    extracted_net_flow_edge['@super target'] = {}\n",
    "    extracted_net_flow_edge[target_node]['@super target'] = 1000000\n",
    "    extracted_net_flow_edge['@super target'][target_node] = 1000000\n",
    "\n",
    "    node2i = {}\n",
    "    i2node = {}\n",
    "    for s in extracted_select_node_set:\n",
    "        nnode = len(node2i)\n",
    "        i2node[nnode] = s\n",
    "        node2i[s] = nnode\n",
    "    if target_node not in node2i:\n",
    "        return [],set(),{}\n",
    "    edge_list, node_list = run_network_flow('tmp',source_node,extracted_select_node_set,extracted_net_flow_edge,node2i,i2node)\n",
    "    return edge_list, node_list, extracted_net_flow_edge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 996,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('dach1', 6313.713174741137), ('e2f1', 50.457841580560526), ('cell cycle', 0.33949684195946794), ('transcription factor', 0.20332001223212323)]\n"
     ]
    }
   ],
   "source": [
    "w1 ='lung adenocarcinoma'\n",
    "w2 = 'tfdp2'\n",
    "sc = {}\n",
    "for w in wct:\n",
    "    if w in ect[w1] and w in ect[w2] and w not in stop_words:\n",
    "        #print w,ect[w1][w]*ect[w2][w]*1.0/wct[w2]/wct[w]/wct[w1]\n",
    "        sc[w] = ect[w1][w]*ect[w2][w]*1.0*239833652*239833652/wct[w2]/wct[w]/wct[w1]/wct[w]\n",
    "sort_x = sorted(sc.items(),key=operator.itemgetter(1))   \n",
    "sort_x.reverse()\n",
    "print sort_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1026,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 1026,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d = 'thyroid'\n",
    "g = 'plcb1'\n",
    "print result_dir + d+'_'+g+\".pdf\"\n",
    "IFrame(result_dir + d+'_'+g+\".pdf\", width=1200, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-331-875dc29ccf59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_node\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mnew_node\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-657-7f9f18182fa8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0msource_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     net_flow_edge,node2i,i2node,select_node_set = extract_sub_graph(select_edge_ct,rev_select_edge_ct,\n\u001b[0;32m---> 21\u001b[0;31m                                                                     source_node,target_set,3)  \n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msource_node\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mselect_node_set\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-632-0d3a524b2a66>\u001b[0m in \u001b[0;36mextract_sub_graph\u001b[0;34m(select_edge_ct, rev_select_edge_ct, source_node, target_set, max_path_L, max_edge)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mdis2t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtarget_set\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mdis2t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdis2t\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdis2target\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mtotal_dis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdis2s\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdis2t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtotal_dis\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmax_path_L\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if not os.path.exists(result_dir):\n",
    "    os.makedirs(result_dir)\n",
    "dct = 0\n",
    "for d in d2g:\n",
    "    dct = dct + 1\n",
    "    #if d!='asthma':\n",
    "    #    continue\n",
    "    if d not in select_edge_ct:\n",
    "        continue\n",
    "    target_set = set()\n",
    "    #sort_x = sorted(d2g[d].items(),key=operator.itemgetter(1)) \n",
    "    #sort_x.reverse()\n",
    "    #for i in range(min(len(sort_x),topk_tgt)):\n",
    "    #    target_set.add(sort_x[i][0])\n",
    "    for g in background_gene:\n",
    "        if g in select_edge_ct:\n",
    "            target_set.add(g)\n",
    "    #target_set = background_gene\n",
    "    source_node = d\n",
    "    net_flow_edge,node2i,i2node,select_node_set = extract_sub_graph(select_edge_ct,rev_select_edge_ct,\n",
    "                                                                    source_node,target_set,3)  \n",
    "    if source_node not in select_node_set:\n",
    "        continue\n",
    "    out_fname = result_dir + d\n",
    "    edge_list, node_list = run_network_flow(out_fname,source_node,select_node_set,net_flow_edge,node2i,i2node)\n",
    "    fout = open(out_fname + '_sentence.txt','w')\n",
    "    fout_backgroud  = open(out_fname + '_sentence_candidates.txt','w')\n",
    "    nedge = len(edge_list)\n",
    "    print d,nedge\n",
    "    if nedge == 0:\n",
    "        continue\n",
    "    dot = Digraph(comment='Network flow')\n",
    "    for node in node_list:\n",
    "        if node==source_node:\n",
    "            dot.attr('node',color='blue')\n",
    "            dot.node(node)\n",
    "        elif node in d2g[d]:\n",
    "            dot.attr('node',color='red')\n",
    "            dot.node(node)            \n",
    "        dot.attr('node',color='black')\n",
    "        dot.node(node)                \n",
    "    for e in edge_list:        \n",
    "        e1,e2,w,mw = e\n",
    "        if e1 in kg_relation and e2 in kg_relation[e1]:\n",
    "            print 'find'\n",
    "            label = 'Percha et al.'\n",
    "        else:\n",
    "            label = 'text mining'\n",
    "        if e1 not in node_type:\n",
    "            t1 = 'entity'\n",
    "        else:\n",
    "            t1 = node_type[e1]\n",
    "        if e2 not in node_type:\n",
    "            t2 = 'entity'\n",
    "        else:\n",
    "            t2 = node_type[e2]\n",
    "        dot.edge(e1,e2,label=label+'('+t1+'-'+t2+':'+str(w)+'))\n",
    "        if e1!='@super target' and e2!='@super target':\n",
    "            #sent = extract_sent(ect_ref,e1,e2)\n",
    "            sent,path,sent_sc,path_l = find_sent(e2,e1)\n",
    "            print e1,e2,sent,path,sent_sc\n",
    "            fout.write(e1+'->'+e2+'\\t'+str(sent_sc)+'\\n'+path+'\\n'+sent+'\\n')\n",
    "            fout_backgroud.write(e1+'->'+e2+'\\t'+str(sent_sc)+'\\n'+path+'\\n'+sent+'\\n')\n",
    "            for p in path_l:\n",
    "                fout_backgroud.write(p+'\\n')\n",
    "            fout.write('\\n')\n",
    "            fout_backgroud.write('\\n')\n",
    "    dot.render(result_dir + d+'_fig') \n",
    "    fout.close()\n",
    "    fout_backgroud.close()\n",
    "    #IFrame(\"test.pdf\", width=600, height=300)\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 995,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "239833652\n"
     ]
    }
   ],
   "source": [
    "sent_ct = 0\n",
    "for pid in range(400):\n",
    "    fin = open('../../data/pubmed/pmid2meta_autophrase.chunk'+str(pid))\n",
    "    ct = 0\n",
    "    for line in fin:\n",
    "        ct = ct + 1\n",
    "        sl = line.strip().split('.')\n",
    "        sent_ct += len(sl)\n",
    "    fin.close()\n",
    "    print pid\n",
    "print sent_ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T03:04:00.016356Z",
     "start_time": "2018-03-06T03:04:00.012456Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 s\n",
      "1 b\n",
      "2 e\n"
     ]
    }
   ],
   "source": [
    "a={[}'s','b','e'}\n",
    "for i,j in enumerate(a):\n",
    "    print i,j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New heading"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
